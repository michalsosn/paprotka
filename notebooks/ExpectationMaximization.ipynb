{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation Maximization\n",
    "\n",
    "[This pdf](https://github.com/mtomassoli/information-theory-tutorial) helped me a lot, but mainly with other stuff than EM. I also found a great blog [here](https://nipunbatra.github.io/blog/2014/em.html). Also, [this](https://www.cs.utah.edu/~piyush/teaching/EM_algorithm.pdf) explanation is more mathy.\n",
    "\n",
    "Iterative method, two steps:\n",
    "* Expectation - Creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters. What are calculated in the first step are the fixed, data-dependent parameters of the function Q.\n",
    "* Maximization - Computes parameters maximizing the expected log-likelihood found on the E step. Once the parameters of Q are known, it is fully determined and is maximized in the second (M) step of an EM algorithm.\n",
    "\n",
    "Other methods exist to find maximum likelihood estimates, such as gradient descent, conjugate gradient, or variants of the Gaussâ€“Newton algorithm. Unlike EM, such methods typically require the evaluation of first and/or second derivatives of the likelihood function.\n",
    "\n",
    "1. First, initialize the parameters $\\boldsymbol {\\theta }$ to some random values.\n",
    "2. Compute the probability of each possible value of $\\mathbf {Z}$, given $\\boldsymbol {\\theta }$.\n",
    "3. Then, use the just-computed values of $\\mathbf {Z}$  to compute a better estimate for the parameters ${\\boldsymbol {\\theta }}$.\n",
    "4. Iterate steps 2 and 3 until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "We have 0 or 1 sample data drawn from two Bernoulli distributions. Variable with Bernoulli distribution takes 1 with probability p and 0 with probability 1 - p, where p is a distribution parameter. We don't know from which distribution a sample comes from.\n",
    "\n",
    "Maximum likelihood estimate of p is a sample mean $\\frac{1}{n} \\sum_i^n x_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools as ft\n",
    "import itertools as it\n",
    "import json\n",
    "import math\n",
    "import operator as op\n",
    "import os\n",
    "\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interact_manual, widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import misc, stats\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = np.array([[1,0,0,0,1,1,0,1,0,1], # the assumption is that each row comes from a single distribution\n",
    "                         [1,1,1,1,0,1,1,1,1,1], # makes sense, without it the solution would probably be that \n",
    "                         [1,0,1,1,1,1,1,0,1,1], # all ones comes from a dist with p=1 and all zeros from p=0\n",
    "                         [1,0,1,0,0,0,1,1,0,0], \n",
    "                         [0,1,1,1,0,1,1,1,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45 0.8\n"
     ]
    }
   ],
   "source": [
    "# if we know from which distributions the observations come from, we can simply calculate \n",
    "# the params from ml estimator\n",
    "\n",
    "distribution_ids = np.array([0, 1, 1, 0, 1]) # hidden params\n",
    "\n",
    "p_0 = observations[distribution_ids == 0].mean()\n",
    "p_1 = observations[distribution_ids == 1].mean()\n",
    "\n",
    "print(p_0, p_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.44914893  0.80498552  0.73346716  0.35215613  0.64721512]\n",
      " [ 0.55085107  0.19501448  0.26653284  0.64784387  0.35278488]]\n",
      "[ 0.71301224  0.58133931]\n"
     ]
    }
   ],
   "source": [
    "# start from some random numbers, you might get different results depending on choice though\n",
    "# visible: model_n, visible_size; hidden: model_n, hidden_size; \n",
    "# observations: observation_n (= hidden_size), observation_size \n",
    "\n",
    "visible_params = np.array([[0.6], [0.5]])\n",
    "\n",
    "def estimate_hidden_params(pmf, observations, visible_params):\n",
    "    hits = observations.sum(axis=1)\n",
    "    observation_n, observation_size = observations.shape\n",
    "    \n",
    "    model_n = visible_params.shape[0]\n",
    "    model_probs = np.zeros((model_n, observation_n), dtype=np.float64)\n",
    "    for i, visible_row in enumerate(visible_params):\n",
    "        prob = pmf(hits, observation_size, visible_row)\n",
    "        model_probs[i] = prob\n",
    "        \n",
    "    total_probs = model_probs.sum(axis=0)\n",
    "    return model_probs / total_probs\n",
    "    \n",
    "hidden_params = estimate_hidden_params(stats.binom.pmf, observations, visible_params)\n",
    "print(hidden_params)\n",
    "\n",
    "def maximize_visible_params(estimate, observations, hidden_params):\n",
    "    per_observation_estimates = [] # observation_n x visible_size\n",
    "    for i, observation_row in enumerate(observations):\n",
    "        visible_row = estimate(observation_row)\n",
    "        per_observation_estimates.append(visible_row)\n",
    "    per_observation_estimates = np.array(per_observation_estimates)\n",
    "        \n",
    "    # (model_n, hidden_size) x (observation_n = hidden_size, visible_size) = (model_n, visible_size)\n",
    "    visible_estimates = hidden_params @ per_observation_estimates / hidden_params.sum(axis=1)\n",
    "    return visible_estimates\n",
    "      \n",
    "visible_params = maximize_visible_params(lambda row: row.mean(), observations, hidden_params)\n",
    "print(visible_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.79678907  0.51958312]\n",
      "[[ 0.10300871  0.95201348  0.84549373  0.03070315  0.6014986 ]\n",
      " [ 0.89699129  0.04798652  0.15450627  0.96929685  0.3985014 ]]\n"
     ]
    }
   ],
   "source": [
    "def expectation_maximization(pmf, estimate, observations, initial_visible, iterations):\n",
    "    visible_params = initial_visible\n",
    "    for i in range(iterations):\n",
    "        hidden_params = estimate_hidden_params(pmf, observations, visible_params)\n",
    "        visible_params = maximize_visible_params(estimate, observations, hidden_params)\n",
    "    return visible_params, hidden_params\n",
    "\n",
    "visible_params, hidden_params = expectation_maximization(\n",
    "    stats.binom.pmf, lambda row: row.mean(), observations, np.array([[0.6], [0.5]]), 1000\n",
    ")\n",
    "print(visible_params)\n",
    "print(hidden_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real values were \n",
    "\n",
    "* visible params [0.45 0.8]\n",
    "* hidden params [0, 1, 1, 0, 1]\n",
    "\n",
    "The algorithms swapped first distribution with second, but it's perfectly fine, they're just identified in different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.83938928,  0.06061072]),\n",
       " array([[ 0.93265475,  0.14600975,  0.93265475,  0.14600975,  0.14600975,\n",
       "          0.93265475,  0.14600975,  0.93265475,  0.93265475,  0.14600975,\n",
       "          0.14600975,  0.14600975,  0.93265475,  0.14600975,  0.93265475,\n",
       "          0.93265475,  0.14600975,  0.93265475,  0.14600975,  0.14600975],\n",
       "        [ 0.06734525,  0.85399025,  0.06734525,  0.85399025,  0.85399025,\n",
       "          0.06734525,  0.85399025,  0.06734525,  0.06734525,  0.85399025,\n",
       "          0.85399025,  0.85399025,  0.06734525,  0.85399025,  0.06734525,\n",
       "          0.06734525,  0.85399025,  0.06734525,  0.85399025,  0.85399025]]))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expectation_maximization(\n",
    "    stats.binom.pmf, lambda row: row.mean(), \n",
    "    np.array([[1], [0], [1], [0], [0], \n",
    "              [1], [0], [1], [1], [0], \n",
    "              [0], [0], [1], [0], [1], \n",
    "              [1], [0], [1], [0], [0]]), \n",
    "    np.array([[0.90], [0.10]]), \n",
    "    1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.45,  0.45]), array([[ 0.5],\n",
       "        [ 0.5]]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expectation_maximization(\n",
    "    stats.binom.pmf, lambda row: row.mean(), \n",
    "    np.array([[1, 0, 1, 0, 0, \n",
    "               1, 0, 1, 1, 0, \n",
    "               0, 0, 1, 0, 1, \n",
    "               1, 0, 1, 0, 0]]), \n",
    "    np.array([[0.75], [0.25]]), \n",
    "    1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.45]), array([[ 1.]]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expectation_maximization(\n",
    "    stats.binom.pmf, lambda row: row.mean(), \n",
    "    np.array([[1, 0, 1, 0, 0, \n",
    "               1, 0, 1, 1, 0, \n",
    "               0, 0, 1, 0, 1, \n",
    "               1, 0, 1, 0, 0]]), \n",
    "    np.array([[0.75]]), \n",
    "    1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.50237364,  0.50217056,  0.49782944,  0.49762636]),\n",
       " array([[ 0.25356136,  0.25118595,  0.2488122 ,  0.24644049],\n",
       "        [ 0.2532549 ,  0.25108626,  0.24891559,  0.24674325],\n",
       "        [ 0.24674325,  0.24891559,  0.25108626,  0.2532549 ],\n",
       "        [ 0.24644049,  0.2488122 ,  0.25118595,  0.25356136]]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expectation_maximization(\n",
    "    stats.binom.pmf, lambda row: row.mean(), \n",
    "    np.array([[1, 0, 1, 1, 1], \n",
    "              [1, 0, 1, 1, 0], \n",
    "              [0, 0, 1, 0, 1], \n",
    "              [1, 0, 0, 0, 0]]), \n",
    "    np.array([[0.8], [0.6], [0.4], [0.2]]), \n",
    "    10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.39894228,  0.26608525])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = lambda row: stats.norm.pdf(1, row)\n",
    "foo(4, 10, np.array([0.1, 10]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
