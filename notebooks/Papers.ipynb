{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep neural networks in speaker recognition\n",
    "### pl-dydaktyka-kodrzywolek_praca_magisterska_skrot.pdf\n",
    "\n",
    "Parametry dźwięku: Voice Activity Detection (oczyszczenie nagrania z fragmentów ciszy, zostawienie nagrań mowy), podział na ramki 20ms, potem MFCC, Filter Banks lub Perceptual Linear Prediction PLP bazująca na Linear Predictive Coding LPC.\n",
    "\n",
    "Modelowanie: Modele wzorcowe vector quantization, dynamic time warping, nearest neighbors. Modele stochastyczne Gaussian Mixture Models, Hidden Markov Models. Stochastyczne raczej się stosuje, bo są bardziej pojemne.\n",
    "\n",
    "Klasyfikacja: We wzorcowych modelach odległość od wzorca, w statystycznych prawdopodobieństwo pochodzenia próbki z modelu.\n",
    "\n",
    "GMM-UBM: Trenujemy model tła UBM za pomocą EM na wszystkich dostepnych nagraniach. Potem generowany jest model użytkownika przez adaptację UBM. Adaptacja zachowuje macierz kowariancji, lecz średnie modelu są przesuwane, by zmaksymalizować prawdopodobieństwo uzyskania wzorców danego mówcy. Weryfikacja przez policzenie wiarygodności\n",
    "\n",
    "$$log P(U | \\lambda_m) - log P(U | \\lambda_{UBM})$$ gdzie U to próbka głosu, $\\lambda_m$ to parametry modelu mówcy m, a $\\lambda_{UBM}$ to parametry modelu tła.\n",
    "\n",
    "i-vectors: Rozwinięcie GMM-UBM. Wektory średnich dla wszystkich komponentów GMM dla danego mówcy są konkatenowane, daje to superwektor. Można no go rozbić na cechy wspólne dla mówców i zależne od mówcy $s = m + T i$ gdzie $m$ to superwektor uzyskany z modelu tła UBM, $T$ to macierz total variability, a $i$ to i-vector opisujący danego.\n",
    "\n",
    "i-vectory dobrze opisano w Dehak N., Shum S.,Low-dimensional speech representation based on factor analysis and its appli-cations, Johns Hopkins CLSP Lecture, 2011\n",
    "\n",
    "Ewaluacja: EER, Detection Cost Function DCF = 0.99 FPR + 0.1 FNR. Histogramy wiarygodności dla targetu/impostora. Detection Error Tradeoff DET zależnośc między FPR na osi x i FNR na osi y\n",
    "\n",
    "Etapy nauki: Pretrening warstwami. Potem dotrenowanie jako jednej sieci MLP lub Stacked Denoising Autoencoders lub Deep Belief Networks. ReLU + Dropout pozwala zrezygnować z pretreningu. Gdy danych z labelami jest bardzo mało wciąż stosuje się pretrening z większym zbiorem bez labelek i potem dotrenowanie z labelkami.\n",
    "\n",
    "Funkcja aktywacji dla ReLU ma warianty: Zwykły ReLU gdzie neuron umiera po dotarciu do 0, leaky max(ax, x) gdzie a ~ 0.01, PReLU gdzie a jest osobne dla każdego neuronu i uczone, Exponential Linear Unit.\n",
    "\n",
    "Autoenkodery - denoising polegają na nakładaniu 0-1 maski na wejściowy wektor. $r ~ Bernoulli(p), x' = r_n \\times r$. Można też powiązać wagi enkodera i dekodera $W_d = W_e^T$. Można też dać im wiele warst i uzyskać Stacked Autoencoder.\n",
    "\n",
    "LSTM - wspomniał, nie opisał prawie\n",
    "\n",
    "Co do DNN: Zamiast tworzyć sieć bezpośrednio do SV raczej używa się sieci wytrenowanej do zmodyfikowanego zadania i wykorzystuje się ją jako część klasyfikatora.\n",
    "\n",
    "stary [30], trudny [31], \"Kolejna [33] wykorzystała sieci Deep Belief Network, jako ekstraktor pseudo i-vectorów, czyli wektorów, które miały takie samo zadanie, jednak nie pochodziły z oryginalnej metody. Siecc składała się z 5 warstw ukrytych, z których każda zawierała 1000 neuronów. Korzystała z wektorów MFCC jedenastu kolejnych ramek sygnału dzwiekowego, a pseudo i-vectory były z sieci wydobywane jako statystyki z aktywacji jej ostatniej warstwy. Praca [34] natomiast wzorowała sie na GMM-UBM. Z wszystkich dostepnych i-vectorów wytrenowano model DBN i nazwano go Universal DBN, na wzór oryginalnego UBM. Nastepnie dla kazdego rejestrowanego mówcy tworzono jego własny model DBN przez dotrenowanie (adaptacje) modelu Universal DBN nagraniami tego mówcy. Najpierw uczeniem nienadzorowanym, a pózniej dyskryminatywnie dla klas target-impostor, w których klasa target reprezentowała tylko i wyłacznie danego mówce. Powyzsze prace osiagały zblizona skuteczność do metody i-vectorów, jednak ̇zadnej nie udało się jej przewyższyć\"\n",
    "\n",
    "Bottleneck Features w [35, 36, 37]\n",
    "\n",
    "Rewelacyjne wyniki w [38]. 7-warstwowa RBM, dla nagrania bierze się wyjścia z jednej warstwy, uśrednia dla całej wypowiedzi i traktuje to jako feature całego nagrania. Na koniec korzysta się z klasyfikatorów CDS, LDA, PLDA. Wynik był zajebisty bo wytrenowali LDA i PLDA na zbiorze testowym. xd\n",
    "\n",
    "Jest też model Google [39, 40]. Weryfikacja mówcy na podstawie frazy _Ok, google_. Model bezpośredni, bo mają ogromne dane. Filter Bank z dźwięku. Potem N ramek do LSTM, na wyjściu wektor cech mówcy. Potem porównanie przez CDS (Cosine distance similarity) z wektorami z rejestracji. Wektory mówcy powstają przez uśrednienie wektorów dla wypowiedzi.\n",
    "\n",
    "d-vector = wektor reprezentujący wypowiedź uzyskany z DNN.\n",
    "\n",
    "Autor połączyć [38] i [40]. Tworzy i-vector tradycyjnie, potem i-vector + kilka ramek nagrania daje się na wejście DNN. Zostanie wypróbowane kilka DNN. Bierze się wektor aktywacji jednej z warstw DNN, uśrednia dla całej wypowiedzi i redukuje wymiary. d-vector porównuje się z rejestracyjnymi.\n",
    "\n",
    "Bazowa parametryzacja 13 cech PLP, wraz z cechami delta oraz delta-delta. Łącznie 39 na ramkę. Używa po 5 sąsiadujących ramek z każdej strony, kontekst 11 ramek, razem wektor ma 429 parametrów.\n",
    "\n",
    "Redukcja wymiarowości najpierw unsupervised PCA do 400, potem supervised LDA do 200.\n",
    "\n",
    "Klasyfikacja - podobieństwo cosinusowe m-dzy d-vectorem i d-vectorem zrobionym z kilku nagrań rejestracyjnych.\n",
    "\n",
    "* 27 - Sak H., Senior A.W., Rao K., Beaufays F.,Fast and accurate recurrent neural network acousticmodels for speech recognition, CoRR, abs/1507.06947, 2015.\n",
    "* 28 - Farrell K.R., Mammone R.J., Assaleh K.T.,Speaker recognition using neural networks and conven-tional classifiers, IEEE Transactions on Speech and Audio Processing, 2(1), 194–205, 1994\n",
    "* 29 - Wouhaybi R.H., Al-Alaoui M.A.,Comparison of neural networks for speaker recognition, Electro-nics, Circuits and Systems, 1999. Proceedings of ICECS’99. The 6th IEEE International Conferen-ce on, tom 1, 125–128, IEEE, 1999.\n",
    "* 30 - Konig Y., Heck L., Weintraub M., Sonmez K., et al.,Nonlinear discriminant feature extractionfor robust text-independent speaker recognition, ESCA workshop on Speaker Recognition and itsCommercial and Forensic Applications, 72–75, 1998.\n",
    "* 31 - Lee H., Pham P., Largman Y., Ng A.Y.,Unsupervised feature learning for audio classification usingconvolutional deep belief networks, Advances in neural information processing systems, 1096–1104, 2009.\n",
    "* 32 - Senoussaoui M., Dehak N., Kenny P., Dehak R., Dumouchel P.,First attempt of boltzmann machi-nes for speaker verification., Odyssey, 117–121, 2012.\n",
    "* 33 - Vasilakakis V., Cumani S., Laface P.,Speaker recognition by means of deep belief networks, 2013.\n",
    "* 34 - Ghahabi O., Hernando J.,Deep belief networks for i-vector based speaker recognition, Acoustics,Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, 1700–1704,IEEE, 2014\n",
    "* 35 - Richardson F., Reynolds D., Dehak N.,A unified deep neural network for speaker and languagerecognition, arXiv preprint arXiv:1504.00923, 2015.\n",
    "* 36 - Richardson F., Reynolds D., Dehak N.,Deep neural network approaches to speaker and languagerecognition, Signal Processing Letters, IEEE, 22(10), 1671–1675, 2015.\n",
    "* 37 - Tian Y., Cai M., He L., Lu J.,Investigation of bottleneck features and multilingual deep neuralnetworks for speaker verification, INTERSPEECH, 1151–1155, ISCA, 2015.\n",
    "* 38 -  Liu Y., et al.,Deep feature for text-dependent speaker verification, ScienceDrect, 2015.\n",
    "* 39 - Chen Y., Lopez-Moreno I., Sainath T.N., Visontai M., Alvarez R., Parada C.,Locally-connected andconvolutional neural networks for small footprint speaker recognition., INTERSPEECH, 1136–1140, ISCA, 2015.\n",
    "* 40 - Heigold G., Moreno I., Bengio S., Shazeer N.,End-to-end text-dependent speaker verification,CoRR, abs/1509.08062, 2015\n",
    "* 44 - Hinton G.,A practical guide to training restricted boltzmann machines, Momentum, 9(1), 926,2010.\n",
    "* 45 - Bengio Y.,Practical recommendations for gradient-based training of deep architectures, NeuralNetworks: Tricks of the Trade, 437–478, Springer, 2012\n",
    "* 46 - Larochelle H., Bengio Y., Louradour J., Lamblin P.,Exploring strategies for training deep neuralnetworks, The Journal of Machine Learning Research, 10, 1–40, 2009.\n",
    "* 49 - Cumani S., Glembek O., Brümmer N., De Villiers E., Laface P.,Gender independent discriminativespeaker recognition in i-vector space, Acoustics, Speech and Signal Processing (ICASSP), 2012IEEE International Conference on, 4361–4364, IEEE, 2012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M. H ́ebert,Springer Handbook of Speech Processing.Berlin,Heidelberg:Springer   Berlin   Heidelberg,    2008,    ch.   Text-Dependent Speaker Recognition, pp. 743–762\n",
    "Old. Hard to find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN based Speaker Recognition on Short Utterances\n",
    "### https://arxiv.org/pdf/1610.03190.pdf\n",
    "\n",
    "In SR systems, number of samples required for enrollment limits practical usage and JFA, i-vectors, PLDA were developed to help. There are attempts to use DNNs in 2 ways: extract bottleneck features BN; senone-replacements where DNNs are used in place of GMM for calculating emission probabilities in HMMs (senone = emission)\n",
    "\n",
    "The aim is to measure the effect of using short (15s) enrollment/verification utterances instead of longer (2min) on DNN-senone GPLDA (Gaussian Probabilistic Linear Discriminant Analysis) SV system.\n",
    "\n",
    "In typical i-vector approach UBM is trained using EM algorithm and has C Gaussian components. It is used to extract 0th/1st/2nd order Baum-Welch (sufficient) statistics.\n",
    "\n",
    "DNN system looks like this:\n",
    "\n",
    "> ASR features -> DNN - frame posteriors -> (+ MFCC features z boku) -> i-vector extractor -> i-vectors\n",
    "\n",
    "ASR = automatic speech recognition. DNN trained using transcribed training set. After extracting i-vectors the system works like GMM-UBM GPLDA system.\n",
    "\n",
    "DNN is Time Delay NN (description in \"Time delay deep neural network-based...”). Uses 40 MFCC with frame 25ms. Cepstral mean substraction over 6 seconds. TDNN has 6 layers, splicing configuration was used as in (A time delayneural network architecture for efficient modeling of longtemporal  contexts,). TDNN was trained using 300 hours of Fisher data. All NIST 2004/5/6/8 data were used for GPLDA training. Kaldi toolkit used.\n",
    "\n",
    "DNN-senone GPLDA system got 50% and 18% improvement over GMM-UBM GPLDA on NIST2010. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TIME DELAY DEEP NEURAL NETWORK-BASED UNIVERSAL BACKGROUND MODELSFOR SPEAKER RECOGNITION\n",
    "### http://ieeexplore.ieee.org/document/7404779/\n",
    "\n",
    "They try to replace GMM with Time Delay NN. Usually DNN are used in SR systems to collect sufficient statistics for i-vector extraction. This DNN is usually trained as the acoustic model in ASR and then repurposed for SR. DNN provides 'alignments for phonetic content', they are combined with standard features and used for i-vector extraction. \n",
    "\n",
    "DNN have increased complexity over GMM which limits practical applications. Ideally GMM could be developed that has ~ posterior as DNN.\n",
    "\n",
    "PLDA backends trained on SRE. TDNN trained on 1800h of Fisher. TDNN described in (A  time  delay  neural  network  architecturefor   efficient   modeling   of   long   temporal   contexts,). 40MFCC without cepstral truncation, frame 25ms. Mean subtraction over 6s. Splicing configuration as in -||-, left context of 13 and right context of 9. 2-norm activation in hidden layers, input dim 350 output dim 3500. Softmax output with  5297 triphone states.\n",
    "\n",
    "Baseline UBM system is trained ..., don't think I need it. They also train supervised GMM to copy TDNN.\n",
    "\n",
    "System based on (Improving  speaker  recognitionperformance in the domain adaptation challenge usingdeep neural networks). Uses TDNN to create UBM. TDNN features in conjunction with SR featuers create the sufficient statistic for i-vector extraction.\n",
    "\n",
    "\n",
    "TDNNs gave huge improvements on SRE10 task. TDNN-UBM best performance reported ever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPROVING SPEAKER RECOGNITION PERFORMANCE IN THE DOMAIN ADAPTATIONCHALLENGE USING DEEP NEURAL NETWORKS\n",
    "### http://ieeexplore.ieee.org/document/7078604/\n",
    "\n",
    "i-vector SR systems use GMM to collect SS. Replacing GMM with DNNs give good results. \n",
    "\n",
    "DNN trained on out-of-domain data. i-vector systems with PLDA require 10ks cuts from 1ks of speakers. There might not be so much data in practice. It could be alleviated by using out-of-domain system to product good results in a new domain for which only unlabeled data is available.\n",
    "\n",
    "Two ways of using out-of-domain systems were developed during some workshop: parameter adaptation and compensation techniques. This paper uses unsupervised parameter adaptation and uses approach from (Unsupervised  domain  adaptation  for  i-vectorspeaker recognition,)\n",
    "\n",
    "Same thing as above... replaced GMM with DNN. This should be described in (“A  novelscheme  for  speaker  recognition  using  a  phonetically-awaredeep neural network). \n",
    "\n",
    "Ehh, let's just skip it and hope we finally get to the fucking description of the DNN. I went through like 3 papers are referencing me to the DNN description somewhere else, wtf.\n",
    "\n",
    "The system trains DNN on transcribed data so they know the content and then can compare speakers producing same content.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks for extractingBaum-Welch statistics for Speaker Recognition\n",
    "### http://cs.uef.fi/odyssey2014/program/pdfs/28.pdf\n",
    "### http://www.crim.ca/perso/patrick.kenny/stafylakis_Odyssey_14_presentation.pdf (presentation)\n",
    "\n",
    "MFCC, PLP don't capture the knowledge of the language. senone = tied triphone state\n",
    "\n",
    "Instead of training UBM with EM, the components are predefined and correspond to the set of triphone states. Posterior probs of them are modeled by DNN.\n",
    "\n",
    "MFCC + D + DD features are great, but you can get even better results by adding high level features. DNNs are better than GMM primarily because they can handle longer segments of speech (300ms), which is long enough to assign them to phonetic classes (like triphone states)\n",
    "\n",
    "In (Speaker recogni-tion by means of Deep Belief Networks) DBN used to extract i-vectors, in (Preliminary investigation of Boltzmann machine classifiersfor speaker recognition) BM used as a backend.\n",
    "\n",
    "First-order statistics are redundant and sparse for each frame. Acoustic events (eg. nasals and fricatives) occur at different times. But when pooled together they stop being sparse and thanks to their sparsity they don't interfere with each other. However when pooled the information about their sequence is lost, but it doesn't matter for Text-Independent SR. Pooling yields fixed size output, which makes creating backend simpler.\n",
    "\n",
    "DNN are used to capture triphones rather than acoustic features.\n",
    "\n",
    "(New types of Deep Neu-ral  Network  Learning  for  speech  recognition  and  relatedapplications:  An overview) and (Sequence-discriminative  training  of  deep  neural  networks,) may have definitions of the DNN\n",
    "\n",
    "DNN: Frames 20-30ms. Input = frame + 5 left + 5 right. Frames = LDA projections of 7 consecutive 13-dim static MFCCs. (3 on each side) This is considered ~ as adding delta and delta-delta MFCC features. Cepstral mean substraction applied prior to LDA projection. After LDA semi-tied covariance transform :o. Finally fMLLR (feature-space maximum likelihood linear regression) is applied. \n",
    "\n",
    "DNN is trained to provide posterior probabilities for HMM states given observations, where states are triphones. DNN output is softmax. Remaining layers sigmoid. Optimal 7 layers.\n",
    "\n",
    "Training: backprop with cross-entropy cost function. This means it minimizes $\\sum_{u=1}^U \\sum_t^{T_y} log y_{t,u}(s_{t,u})$ where $s_{t,u}$ is state at time t of u utterance. DNN can be pre-trained with stacked RBM and unlabeled data, but this can be skipped if you have a ton of labeled data.\n",
    "\n",
    "Switchboard (SWB) and CallHome English (CHE) benchmarks\n",
    "\n",
    "\"HMM with language model is only used in the first pass to estimate the fMLLR transform\" DNN posteriors are estimated without the use of HMM or language model. This implementation is not optimal, but it's a good starting point.\n",
    "\n",
    "OHUI JEST CO TO 0/1 ORDER BAUM WELCH STATISTICS = to to z pdfa który mam obok = jaaj\n",
    "\n",
    "![schema](images/dnn-ivector-schema.png)\n",
    "\n",
    "DNN: \n",
    "\n",
    "* Several hidden layers (5-6) and triphone states as outputs\n",
    "* They are discriminative classifiers, yet they are combinedwith HMMs\n",
    "* Initialized with stacked RBMs (not any longer in ASR)\n",
    "* Capacity in handling longer segments as inputs (about300ms)\n",
    "\n",
    "Their performance was lower to traditional i-vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW TYPES OF DEEP NEURAL NETWORK LEARNING FOR SPEECH RECOGNITION AND RELATED APPLICATIONS: AN OVERVIEW \n",
    "### http://ieeexplore.ieee.org/document/6639344/\n",
    "\n",
    "Overview of papers on: better optimization, better activation functions and models, better ways of determining hyperparams, better speech preprocessing, ways of using multiple languages.\n",
    "\n",
    "DNN-HMM (in ) and HTM (Hidden Trajectory Model)\n",
    "\n",
    "DNNs work better on Filterbank outputs that MFCCs. ReLU + Dropout >> logistic.\n",
    "\n",
    "Hyperparameters: Bayesian optimization ([50], sampling [3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network Approaches to Speaker and Language Recognition\n",
    "### https://groups.csail.mit.edu/sls/publications/2015/Dehak_IEEE-2015.pdf\n",
    "\n",
    "There are 2 approaches to using DNN in SR: Use DNN trained as a classifier for recognition task directly ([5] [11] - frame level features) or use a DNN trained for different purpose to extract features that are then used by another classifier. ([2] [3] [12]; [7] [11] - multinomial or Gaussian vector; [13] [14] multi-modal stats for i-vectors)\n",
    "\n",
    "Direct method might be unusable for us because it requires a large dataset and cannot cross train.\n",
    "\n",
    "So one way you can use DNN is calculate i-vectors in the same way as using GMM-UBM, but replacing UBM posteriors with DNN posteriors. (Old i-vector paper [13]. DNN in [6], [7], [17])\n",
    "\n",
    "Other way is using bottleneck features. Extract features for use by second classifier by taking outputs of a hidden layer of another DNN.  (In [10], [12], [18])\n",
    "\n",
    "OHUI seems to have a nice description of i-vectors again...\n",
    "First tested direct 2 layer DNN. It's quite clearly described too! They used NIST 120 hours. It got worse EER on 30 and 10s, slightly better on 3s.\n",
    "\n",
    "Then 7 layer BNF DNN. Trained on Switchboard1 (not on Fisher!) 100h. BNF work well, DNN posterior give worse results.\n",
    "\n",
    "Either BNF/GMM or MFCC/DNN work well. And their mixtures even better on some tasks.\n",
    "\n",
    "BNF/GMM model did great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AANN: an alternative to GMM for pattern recognition\n",
    "\n",
    "GMM representation is constrained because of: shape of gaussian distribution + fixed number of mixtures \n",
    "\n",
    "old, skip for now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-discriminative training of deep neural networks\n",
    "### http://www.danielpovey.com/files/2013_interspeech_dnn.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADVANCES IN DEEP NEURAL NETWORK APPROACHES TO SPEAKER RECOGNITION\n",
    "### http://ieeexplore.ieee.org/document/7178885/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A NOVEL SCHEME FOR SPEAKER RECOGNITION USING A PHONETICALLY-AWAREDEEP NEURAL NETWORK\n",
    "### http://ieeexplore.ieee.org/document/6853887/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks for Acoustic Modeling in Speech Recognition\n",
    "### http://ieeexplore.ieee.org/document/6296526/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNSUPERVISED DOMAIN ADAPTATION FOR I-VECTOR SPEAKER RECOGNITION\n",
    "### https://groups.csail.mit.edu/sls/publications/2014/garcia-romero_odyssey2014.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Dependent  - Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A NOVEL SCHEME FOR SPEAKER RECOGNITION USING A PHONETICALLY-AWAREDEEP NEURAL NETWORK\n",
    "### http://ieeexplore.ieee.org/document/6853887/\n",
    "\n",
    "i-vector model driven by DNN trained for ASR. DNN replace GMM to produce frame alignments. This usage allows traditional backends to remain unchanged. They used in on NIST, which is a TI task. There should be a way to leverage transcribed data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AN ITERATIVE DEEP LEARNING FRAMEWORK FOR UNSUPERVISED DISCOVERY OFSPEECH FEATURES AND LINGUISTIC UNITS WITH APPLICATIONS ON SPOKEN TERMDETECTION\n",
    "### https://arxiv.org/pdf/1602.00426.pdf\n",
    "\n",
    "Unlabeled speech data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEEP NEURAL NETWORKS BASED SPEAKER MODELING AT DIFFERENT LEVELS OF PHONETIC GRANULARITY\n",
    "\n",
    "They take model with i-vectors and DNN trained to predict tied-triphone states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network based Text-Dependent Speaker Recognition:Preliminary Results\n",
    "\n",
    "They checked RNNs and failed.\n",
    "\n",
    "Phoneme-discriminant DNN - classifies phonemes or trinemes. Replace GMM-UBM with DNN. Or bottleneck where DNN is supervised-trained to detect triphones as targets. Then deep features are extracted for each frame. And used as features in GMM-UBM or PLDA. Tandem features may be used, and are MFCC + DNN slice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bottleneck Features  - Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation of Bottleneck Features and Multilingual Deep Neural Networks for Speaker Verification\n",
    "### https://pdfs.semanticscholar.org/ab77/e56b705cde2e6e045102537b905d87c21ad4.pdf\n",
    "\n",
    "Recently, DNN with senone outputs to produce frame alignments for sufficient statistics extractions did good. However it requires transcribed training data. Second the language of DNN is limited to pronounciation in training set which may fail in other languages.\n",
    "\n",
    "> In our method, a DNN is first trained with senone labels to extract bottleneck features. Then a Gaussian mixturemodel (GMM) is trained with the bottleneck features to produce frame alignments.\n",
    "\n",
    "In i-vector, PLDA framework MFCC or PLP (perception inear prediction features) are first converted into high-dim sufficient statistics using the occupancy posterior probabilities generated by GMM-UBM. Then dim-reduces.\n",
    "\n",
    "DNN work well as replacement for GMM-UBM ([5], [6]) because can handle longer sequences and do non-linear mappings.\n",
    "\n",
    "Recently a method was proposed that replaces GMM with DNN to compute frame posterior probs for each class during the computation of SS ([11] [12]). They used transcribed data.\n",
    "\n",
    "In this paper BN features are calculated with DNN, and then GMM is based on them.\n",
    "\n",
    "Frame alignments: use HMM / MFCC, statistic computation: use DNN / FBanks\n",
    "Chyba chodzi o to, by GMMHMM unsupervised znaleźć klasy ramek, ale wtedy nie wiadomo co dana klasa oznacza. Potem wziąć transkrypcję, założyć (?) że kolejne klasy w nagraniu to kolejne fonemy z transkrypcji i nauczyć DNN klasyfikacji fonemów.\n",
    "\n",
    "BN here means a hidden layer placed in the middle of aDNN which has a relative small number of hidden units com-\n",
    "pared to the size of the other layers.\n",
    "\n",
    "DNN tested on NIST SRE SRE2010 fem, SRE2008 fem. Trained on 300h of Switchboard datasets. Mandarin dataset self-collected.\n",
    "\n",
    "> A GMM-HMM is firsttrained to generate transcriptions for senones. The GMM-HMMsystem uses 13-dimensional PLP features with speaker-basedmean-covariance normalization.  The basic features are thenconcatenated with their first-, second- and third-order deriva-tives and further reduced to 39 dimensions by HLDA. The DNNused to provide the posterior probability has five-hidden layersand is trained with cross-entropy criterion using the transcrip-tions from the GMM-HMM. The input layer of the DNN has1320 nodes composed of 11 frames (5 frames on each side ofthe frame) where each frame consists of 120 log Mel-filterbankcoefficients (40 basic + first order + second order). Each hidden layer has 1200 nodes. The configurations of the DNN with BNlayer are the same except that the number of the fifth hiddenlayer is changed to 39. Different number of output senones areevaluated and the details are in the results section\n",
    "\n",
    "Hmm...  A GMM-HMM is firsttrained to generate transcriptions for senones. DNN is trained with cross-entropy criterion using the transcrip-tions from the GMM-HMM. \n",
    "Hmm, to mega słabo :/ Nie używamy przecież wtedy transkrypcji.\n",
    "\n",
    "In the DNN based systems, DNNs with senone outputs areused to provide frame alignments during SS extraction\n",
    "\n",
    "In the BN features based systems, DNNs with senone out-puts are used to extract BN features. Then a UBM is trainedusing these BN features and used to provide frame posteriorsduring sufficient statistics extraction.\n",
    "\n",
    "Ma to sens, bo DNN nie jest używane do skopiowania posteriora GMM, tylko slice z niego jest brany jako feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google - End to End global password - Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Text-Dependent Speaker Verification\n",
    "### https://static.googleusercontent.com/media/research.google.com/pl//pubs/archive/44681.pdf\n",
    "\n",
    "\"Ok Google\" dataset\n",
    "\n",
    "Text-dependent, short phrase 0.6s, direct approach, simple model, ton of google data\n",
    "THEIR TASK IS WAY EASIER BECAUSE THEY ALWAYS PRESENT OK GOOGLE PHRASE. IT'S HARDLY TEXT-DEPENDENT,\n",
    "THEY CAN ASSUME CONTENT IS ALWAYS THE SAME. \n",
    "\n",
    "Small footprint = less RAM. They seem to be very concerned with applicability of their model eg. it small and fast is better.\n",
    "\n",
    "Phases of SV system:\n",
    "\n",
    "* Training - Find suitable representation of speaker from utterance\n",
    "* Enrollment - take few enrollment samples, build speaker model\n",
    "* Evaluation - calculate score for a new utterance X and selected speaker model spk. Compare with threshold\n",
    "\n",
    "DNN: \n",
    "Locally connected ReLU -> Fully ReLU + -> Fully linear\n",
    "Cost - cross-entropy, softmax based on (w_spk^T y + b_spk)\n",
    "\n",
    "Enrollment - run DNN for all utterances, each utterance gives on d-vector, average all d-vectors for a speaker to get one speaker d-vector. During eval also generate d-vector and use cosine similarity.\n",
    "\n",
    "DNN2 - LSTM:\n",
    "same way of workign as above, but they used LSTM. \n",
    "LSTM layer -> Fully connected\n",
    "They present N enrollments to LST. LSTM has multiple outputs, only take only last. For enrollment this last output is averaged to get d-vector.\n",
    "\n",
    "LSTM can take variable length, unlike first DNN, which needs windowing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locally-Connected and Convolutional Neural Networks for Small FootprintSpeaker Recognition\n",
    "\n",
    "Same as google - they are concerned with single phrase (called \"global password\") case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RedDots Related - Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utterance Verification for Text-Dependent Speaker Recognition: aComparative Assessment Using the RedDots Corpus\n",
    "### http://cs.uef.fi/sipu/pub/1125.pdf\n",
    "\n",
    "Combination of speaker verification and utterance verification give better results than doing them separately.\n",
    "UV is combined with SV in HMM approach.\n",
    "Utterance verification could greatly improve security in text-prompted case. In this case user is given a phrase to say and this information can be used to improve verification. On the other case if the phrase is different each time pre-recording and replaying the speaker would not be possible. \n",
    "\n",
    "In speaker verification evaluate log-likelihood ratio score\n",
    "$$l_s(X, j) = log \\frac{p(X | \\text{same speaker})}{P(X | \\text{different speaker})}$$\n",
    "\n",
    "where X - features of the utterance, j - speaker id.\n",
    "\n",
    "Numerator and denominator can be evaluated with:\n",
    "* adapted target speaker GMM and an universal background model (UBM)\n",
    "* probabilistic linear discriminant analysis (PLDA) with i-vectors as input features\n",
    "\n",
    "In utterance verification\n",
    "$$l_u(Y, k) = log \\frac{p(X | \\text{same text})}{P(X | \\text{different text})}$$\n",
    "\n",
    "where $Y$ - features of the utterance, $k$ - promted text id\n",
    "\n",
    "Same methods could be used as in SV. \n",
    "\n",
    "Author, with assumption that they know all phrases (they use only 10 as in RedDots first case). With this assumption they can subtract from the $l_u$ of the real phase the mean or max score of other, wrong phrases.\n",
    "\n",
    "Combined\n",
    "$$l_{su}(X, Y, j, k) = l_s(X, j) + l_u(Y, k)$$\n",
    "\n",
    "\n",
    "UV1\n",
    "* Their system uses MFCC features and GMM-UBM model. (Gaussian mixture model – Universal background model)\n",
    "* MFCC - 20 filters in Mel scale. Perform RASTA-filtering on 19 coefficients, add deltas and double deltas to get 57-dimensional features. Then cepstral-mean normalization.\n",
    "* UBM - 512 components, trained with all male data from TIMIT\n",
    "* Utterance models obtained with MAP adaptation with relevance factor of 3.\n",
    "* Target-to-UBM log-likelihood is used as the UV score.\n",
    "\n",
    "UV2\n",
    "* 2-layer approach with HMM and UBM\n",
    "* left-to-right 5-state HMM with continuous observation densities modeled with GMM\n",
    "* GMM adapted from a 512-component UBM trained on TIMIT, \n",
    "\n",
    "UV3\n",
    "* DTW to align feature vectors for a pair of utterances, euclidean distance for frames\n",
    "* 57 MFCC without RASTA\n",
    "* average score against all the utterances is used as the score\n",
    "\n",
    "UV4\n",
    "* Forced alignment. Created 10 reference transcripts for the 10 sentences using TIMIT. All test segments were force aligned with the references. \n",
    "\n",
    "Acoustic phone model\n",
    "* MFCC with LDA and feature-space maximum likelihood linear regression fMLLR were used as DNN inputs with left and right contexts of 3 frames.\n",
    "* Training 1: unsupervised training of a stack of RBMs with 1024 hidden units, 6 layers, 13 iterations.\n",
    "* Training 2: DNN with objective to classify individual frames to their probability density functions via cross-entropy objective.\n",
    "* Training 3: Optimize state-level minimum Bayes risk (sMBR) to emphasize state sequences with higher frame accuracy\n",
    "\n",
    "SV1\n",
    "* same as UV1\n",
    "\n",
    "SV 2\n",
    "* same as UV1 but with constant Q cepstral coefficients as input\n",
    "\n",
    "SV 3\n",
    "* Speaker Independent HMM. Then speaker models are derived using SI-HMM and MAP adaptation of Gaussian means using the enrollment data.\n",
    "* During testing the test data is force-aligned against the target model ans SI-HMM and log-likelihood ratio is calculated. \n",
    "* 14 states and 8 mixtures provided best results\n",
    "\n",
    "SV 4\n",
    "* i-vectors S = m + Tw where w is the i-vector, S is utterance supervector, m is the UBM supervector, T is a low-rank matrix. \n",
    "* Gender-dependent GMM-UBM of 512 mixtures trained using 157 male speakers from RSR2015 corprus consisting od 30 pass phrases from 9 sessions (~42k utterances). i-vector dimension 400\n",
    "\n",
    "Did male-only verification because RedDots lack female subjects.\n",
    "\n",
    "Evaluation\n",
    "* EER - error rate when false acceptance probability and false rejection probability are equal\n",
    "* Separately report FAR(TW), FAR(IC), FAR(IW) \n",
    "* UV2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLOITING SEQUENCE INFORMATION FOR TEXT-DEPENDENT SPEAKER VERIFICATION\n",
    "### https://infoscience.epfl.ch/record/225954/files/Dey_Idiap-RR-04-2017.pdf\n",
    "\n",
    "> Model-based approaches to Speaker Verification (SV), such as Joint  Factor  Analysis  (JFA),  i-vector  and  relevance  Maximum-a-Posteriori  (MAP),  have  shown  to  provide  state-of-the-art  perfor-mance  for  text-dependent  systems  with  fixed  phrases. \n",
    "> The  per-formance  of  i-vector  and  JFA  models  has  been  further  enhancedby  estimating  posteriors  from  Deep  Neural  Network  (DNN)  instead of Gaussian Mixture Model (GMM).\n",
    "\n",
    "(from abstract)\n",
    "\n",
    "Most  of  the  techniques  to  tackle  text-dependent  SV  can  begrouped into two main categories: (a) model-based and (b) template-based  (Dynamic  Time  Warping  (DTW))  techniques.\n",
    "\n",
    "PLDA is a method for classifying. It turns dimensionality reduction LDA into a classification method.\n",
    "\n",
    "---\n",
    "JFA!!! http://www1.icsi.berkeley.edu/Speech/presentations/AFRL_ICSI_visit2_JFA_tutorial_icsitalk.pdf\n",
    "s = m + Vy + Ux + Dz\n",
    "speaker supervector = speaker-independent component + speaker-dependent component + channel-dependent component + speaker-dependent residual component\n",
    "m - speaker independent supervector (from UBM)\n",
    "V - eigenvoice matrix\n",
    "y - speaker factors, assumed to have N(0, 1) prior distribution\n",
    "U - eigenchannel matrix\n",
    "x - channel factors, N(0, 1) prior\n",
    "D - residual matrix, diagonal\n",
    "z - speaker specific residual factors, N(0, 1) prior\n",
    "\n",
    "---\n",
    "\n",
    "Paper says you can use MAP adapting of GMM-UBM model, i-vector (s = u + Tw) where u is mean supervector of GMM-UBM or jfa (s = u + Dz + Ux). Using DNN posteriors as i-vectors gives great results.\n",
    "\n",
    "The i-vectors for a recording are often just averaged, which drops the information about in what sequence they occured. Authors suggest that DTW or HMM can be used to retain the information.\n",
    "\n",
    "Trained, evaluated on male recording, part 1 of RedDots dataset.\n",
    "\n",
    "20 MFCC, 25ms frames with 10ms sliding window, appended with delta and acceleration parameters. Short time gaussianization applied to those features, using 3s sliding window. Hungarian phoneme recognizer used to detect voice activity.\n",
    "\n",
    "1024 params of GMM-UBM estimated on ~120h subset of Fisher corprus and Part 1 of RSR (?). i-vector extractor of 400 dims also trained with same training set. JFA rank of eigenchannel matrix is 50. \n",
    "\n",
    "DNN with 4 hidden layers, 1200 sigmoid units per layer, 1530 softmax units at output.\n",
    "\n",
    "> Evaluations  are  done  onthree conditions labeled as Cond1, Cond2, Cond3, and an additionalcondition (Cond-all) with the trials from all three conditions put to-gether. \n",
    "> More particularly, in condition 1, each trial is associated withdetermining if the phrases are the same or different.  In condition 2,the system is required to differentiate speakers pronouncing the samecontent.  In condition 3, both the speaker and the phrase can be different.\n",
    "\n",
    "* RMAP 5.2 4.1 1.0 1.8\n",
    "* IVec(GMM, PLDA) 6.9 4.2 1.3 1.9\n",
    "* JFA(GMM) 10.5 7.9 2.9 3.8\n",
    "\n",
    "\n",
    "* IVec(DNN, PLDA) 6.9 3.4 1.2 1.6\n",
    "* JFA(DNN) 4.1 7.0 1.2 2.5\n",
    "\n",
    "\n",
    "* DTW-MFCC 2.1 5.6 1.2 1.9\n",
    "* DTW-post(GMM) 1.8 6.7 1.7 2.9 \n",
    "* DTW-post(DNN) 1.1 9.0 1.0 3.5 \n",
    "* DTW-onIvec(GMM) 2.6 3.8 1.3 1.8 \n",
    "* DTW-onIvec(DNN) 1.3 3.2 0.8 1.3 \n",
    "* onIvec(GMM, PLDA) 5.4 6.7 2.5 2.9 \n",
    "* onIvec(DNN, PLDA) 2.8 4.8 1.8 2.1\n",
    "\n",
    "(EER in %)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Speaker and Content Modelling for Text-dependent Speaker Verification\n",
    "### https://pdfs.semanticscholar.org/fcb5/af7bbb24e3bea6564e57e3a2cceca6af5481.pdf\n",
    "\n",
    "They used two systems. One system modelling speaker based on known lexical content, othr modelling lexical content based on known speaker. Speaker verification uses HMM GMM, lexical content verification uses GMM. They also created a mixture model based on KL divergence.\n",
    "\n",
    "TD systems showed higher accuraccy that TI ones. Current research focuses on very short utterances, ~1.5s.\n",
    "\n",
    "Advantage of TD models come from knowing the content in advance. Should we even verify the content?\n",
    "\n",
    "Joint Factor Analysis, HMM, GMM-UBM, DNN, LSTM were all used.\n",
    "\n",
    "Front end features - MFCC 19 dimensions with log energy their first and second derivatives. Vector quantization model voice activity detector was used, then feature warping was applied.\n",
    "\n",
    "SV: HMM GMM, N-state, GMM are initialized with UBM. Then trained with all data to estimate background pass-phrase HMM. Then this HMM is copied for each speaker and MAP adapted using their recordings. Scoring: P(O|HMM) are estimated with Viterbi algorithm, final score $S_{HMM} = log(O|\\text{Speaker HMM}) - log(O|\\text{Background pp HMM})$\n",
    "\n",
    "UV: left-to-right segment model. Split each pass phrase into S segments and use separate GMM to model each segment. Test segment is also divided into S segments and score is $S_{seg} = \\frac{1}{S}\\sum_{i=1}^S (log P(O_i|\\lambda_{seg_i}) - P(O_i|\\lambda_{UBM}))$\n",
    "\n",
    "Baseline system is GMM UBM with 19 MFCC, 512 mixtures. Only means of GMM are MAP adapted.\n",
    "\n",
    "Only part 1 males are considered. Seems like extremely common decision. Results for IC, TW, IW are expressed in terms of EER.\n",
    "\n",
    "EER on different subsystems:\n",
    "\n",
    "|    | GMM  | 4seg | 8seg | 4HMM | 8HMM | 8HMM + 4seg |\n",
    "|:-- | ----:| ----:| ----:| ----:| ----:| -----------:|\n",
    "| IC | 2.41 | 2.81 | 5.64 | 1.20 | 1.19 | 1.76        |\n",
    "| TW | 5.11 | 2.78 | 6.29 | 6.42 | 5.92 | 2.72        |\n",
    "| IW | 0.59 | 0.62 | 2.22 | 1.23 | 1.20 | 0.46        |\n",
    "\n",
    "ERR on various mixtures\n",
    "\n",
    "|    | GMM  | MS256 | MS128 | MS64 | MS32 | MS256_4seg | MS256_4seg + 8HMM |\n",
    "|:-- | ----:| -----:| -----:| ----:| ----:| ----------:| -----------------:|\n",
    "| IC | 2.41 | 2.34  | 2.50  | 2.96 | 4.34 | 2.80       | 1.45              |\n",
    "| TW | 5.11 | 4.50  | 3.98  | 4.18 | 5.62 | 2.50       | 2.50              |\n",
    "| IW | 0.59 | 0.48  | 0.52  | 0.77 | 1.24 | 0.56       | 0.37              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPARISON OF MULTIPLE FEATURES AND MODELING METHODS FORTEXT-DEPENDENT SPEAKER VERIFICATION\n",
    "### https://arxiv.org/pdf/1707.04373.pdf\n",
    "\n",
    "TD changes everything! i-vectors not the best, HMM-based GMM-HMM and i-vector/HMM give great results on fixed-phrase, less good when promted. Bottleneck features do not outperform MFCC when TD.\n",
    "\n",
    "TI ASV has very bad results on short utterances. TD can give good results if test phrases are well designed.\n",
    "\n",
    "TD techniques: template-based and statistical-based. \n",
    "\n",
    "Template: DTW. Features must have info both about content and speaker. GMM posteriors and DNN posteriors were used as features ([5] [6]). DNN couldn't discriminate speakers so DNN-based i-vectors were used too ([7]). Metric: KL Div natural when comparing posteriors, cosine used too when working online bc its faster. Templates are good but only on fixed phrases.\n",
    "\n",
    "Statistical: uses techniques from TI SR: GMM-UBM, GMM-SVM, JFA, i-vectors ([9], [10], [11], [2]). To explicitly capture sequence info HMM can be built, either with no prior knowledge or with text transcriptions. ([12]-[15])\n",
    "\n",
    "In TD still GMM-UBM > i-vectors, unlike TI.\n",
    "\n",
    "Bottleneck features are also tried, they are extracted from a hidden layer of a DNN. Two styles of training are used: utterance-level=speaker-embedding ([17]..[19]) or frame-level=combination of speaker and phonemes. ([20], [21]) Speaker-embeddings have an issue that they require a large amount of in-domain data. Thus frame-level will be tested in the paper.\n",
    "\n",
    "Paper compares: GMM-UBM, i-vector, GMM-HMM, i-vector/HMM. When phrase fixed \\*HMM methods outperform first two. In prompted phase case HMM have no advantage. They also compared Viterbi vs FB and FB was a bit better, but Viterbi is probably still preferable because 2x less computations. BN phonetic-dependent features did well in detecting incorred phrase and poorly detecting impostors. BN speaker-discriminant worker poorly altogether.\n",
    "\n",
    "OHUI MAP adaptation, 0/1st order stats, short i-vectors, HMM many rather good descriptions of how they work\n",
    "\n",
    "d-vector = i-vector from speaker-discriminant BN network.\n",
    "\n",
    "They used part 1 and 4 from RedDots. Excluded women because very few in dataset. (49 male vs 13 female) Seem common to exclude women, might consider to do the same... They report all error types (IW IC TW) on 1st part and only IC on 4 part.\n",
    "\n",
    "Librispeech 1000 hours of English speech.\n",
    "\n",
    "Part 1\n",
    "\n",
    "| Modeling             |      | IC     |        |      | TW     |        |      | IW     |        |\n",
    "|:-------------------- | ----:| ------:| ------:| ----:| ------:| ------:| ----:| ------:| ------:|\n",
    "|                      | EER  | MDCF08 | MDCF10 | EER  | MDCF08 | MDCF10 | EER  | MDCF08 | MDCF10 |\n",
    "| GMM-UBM              | 2.23 | 0.0116 | 0.3685 | 6.02 | 0.0278 | 0.6070 | 0.86 | 0.0033 | 0.1758 |\n",
    "| i-vector             | 4.26 | 0.0193 | 0.5079 | 9.99 | 0.0468 | 0.8072 | 1.39 | 0.0065 | 0.2902 |\n",
    "| GMM-HMM Viterbi      | 1.91 | 0.0080 | 0.2649 | 1.91 | 0.0072 | 0.2452 | 0.65 | 0.0016 | 0.0478 |\n",
    "| GMM-HMM FB           | 1.88 | 0.0078 | 0.2483 | 1.91 | 0.0074 | 0.2677 | 0.62 | 0.0015 | 0.0718 |\n",
    "| i-vector/HMM Viterbi | 2.31 | 0.0085 | 0.2316 | 2.00 | 0.0076 | 0.1993 | 0.65 | 0.0016 | 0.0466 |\n",
    "| i-vector/HMM FB      | 2.07 | 0.0081 | 0.2082 | 1.82 | 0.0075 | 0.1829 | 0.59 | 0.0015 | 0.0404 |\n",
    "\n",
    "Part 4\n",
    "\n",
    "| Modeling             | EER   | MDCF08 | MDCF10 |\n",
    "|:-------------------- | -----:| ------:| ------:|\n",
    "| GMM-UBM              |  9.03 | 0.0431 | 0.9857 | \n",
    "| i-vector             | 11.64 | 0.0468 | 0.9611 | \n",
    "| GMM-HMM Viterbi      |  9.78 | 0.0463 | 0.9282 | \n",
    "| GMM-HMM FB           |  9.79 | 0.0460 | 0.9359 | \n",
    "| i-vector/HMM Viterbi |  9.94 | 0.0446 | 0.8453 |  \n",
    "| i-vector/HMM FB      |  9.52 | 0.0438 | 0.8476 | \n",
    "\n",
    "Part 1\n",
    "\n",
    "| Modeling             |      | IC     |        |       | TW     |        |      | IW     |        |\n",
    "|:-------------------- | ----:| ------:| ------:| -----:| ------:| ------:| ----:| ------:| ------:|\n",
    "| MFCC                 | 1.91 | 0.0080 | 0.2649 | 1.91  | 0.0072 | 0.2452 | 0.65 | 0.0016 | 0.0478 | \n",
    "| MFCC + pBN           | 2.75 | 0.0130 | 0.4472 | 1.02  | 0.0038 | 0.1132 | 0.55 | 0.0016 | 0.0768 | \n",
    "| MFCC + sBN           | 8.76 | 0.0550 | 0.9599 | 23.57 | 0.0981 | 0.9954 | 5.71 | 0.0348 | 0.9429 |\n",
    "\n",
    "Part 4\n",
    "\n",
    "| Modeling             | EER   | MDCF08 | MDCF10 |\n",
    "|:-------------------- | -----:| ------:| ------:|\n",
    "| MFCC                 | 9.78  | 0.0463 | 0.9282 | \n",
    "| MFCC + pBN           | 15.04 | 0.0649 | 0.9903 | \n",
    "| MFCC + sBN           | 15.87 | 0.0783 | 0.9703 |\n",
    "\n",
    "* [5] Speakerverification  using  gaussian  posteriorgrams  on  fixed  phraseshort utterances\n",
    "* [6] Deep neu-ral network based posteriors for text-dependent speaker veri-fication\n",
    "* [7] Exploitingsequence information for text-dependent speaker verification\n",
    "* [9] Tandem Features for Text-dependent Speaker Verification on the RedDots Corpus\n",
    "* [10] A new study of gmm-svmsystem for text-dependent speaker recognition\n",
    "* [11] Text-dependentspeaker  recognition  with  random  digit  strings\n",
    "* [2] Text-dependentspeaker   verification:Classifiers,   databases   and   rsr2015\n",
    "* [12] Parallel speaker and content modelling for text-dependent  speaker  verification\n",
    "* [13] Text dependent speaker verifica-tion using un-supervised hmm-ubm and temporal gmm-ubm\n",
    "* [14] Joint speaker  and  lexical  modeling  for  short-term  characterizationof speaker,\n",
    "* [15] Text-dependent speaker verification based on i-vectors, neural net-works and hidden markov models,\n",
    "* [17] End-to-endtext-dependent speaker verification,\n",
    "* [18] Deep neural network-basedspeaker embeddings for end-to-end speaker verification\n",
    "* [19] Deep speaker:  an end-to-end neu-ral speaker embedding system\n",
    "* [20] Deepfeature for text-dependent speaker verification\n",
    "* [21] Deepneural network based text-dependent speaker recognition: Pre-liminary results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tandem Features for Text-dependent Speaker Verification on the RedDots Corpus\n",
    "### http://www.crim.ca/perso/patrick.kenny/Alam_interspeech2016.pdf\n",
    "\n",
    "Frames -> MFCC -> DNN senone-discriminant providing BF at each frame -> GMM UBM\n",
    "\n",
    "They add Linear Frequency Cepstral Coefficients and Wavelet-based Cepstral Coefficients to MFCC\n",
    "\n",
    "Results on 1 and 4 part of Reddots\n",
    "\n",
    "* Text-dependent approaches: DTW, GMM/UBM, Hierarchical multi-Layer Acoustic Model (HiLAM), Hidden Markov Model, Joint Factor Analysis (JFA)  \n",
    "* Text-independent: UBM/i-vector/PLDA system\n",
    "\n",
    "Tandem features = MFCC `concat` BF from senone-discriminant NN, then PCA\n",
    "\n",
    "They also use LFCC, WBCC, I gave full names above\n",
    "\n",
    "Trained DNN on Fisher/Switchboard. Will test on RedDots and LibriSpeech.\n",
    "\n",
    "![RedDots results](images/reddots-results-tandem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i-vector/HMM Based Text-dependent Speaker Verification System for RedDots Challenge\n",
    "### http://www.fit.vutbr.cz/research/groups/speech/publi/2016/zeinali_interspeech2016_IS161174.pdf\n",
    "\n",
    "HMM based i-vector approach. Phone specific HMMs used to collect SS for i-vectors. Compared with standard MFCC and DNN based BF.\n",
    "\n",
    "> Historically, text-independent SV has receivedmore attention due to the regular organization of NIST chal-lenges\n",
    "\n",
    "They expand on their work in (Deep neural networks and hidden Markov models in i-vector-based text-dependent speaker verification,). Previously they focused on detecting impostors when the impostor says right sentence, now they focus on detecting wrong sentences.\n",
    "\n",
    "> Generally, we have seenthat in text-dependent speaker verification, we cannot do anychannel compensation and score normalization as it is usual intext-independent systems. This happens due to very short ut-terances, where phonetic variation is dominant compared to thespeaker and channel variation and so the i-vectors of differentphrases have large distances from each other.\n",
    "\n",
    "Generuje HMM per phrase, hmm. I na podstawie HMM GMM generuje i-vectory w ten sposób, że Viterbim wylicza ścieżkę stanów i w każdej chwili ma jeden wybrany stan i do wyliczania SS do i-vectorów bierze tylko mikstury emisji z danego stanu.\n",
    "\n",
    "> 2 speakers including 49 males and 13 females. 41 speak-ers are used as target speakers (35 males and 6 females) and theother ones are considered as unseen imposters\n",
    "\n",
    "Mogę użyć tych liczb jak je znalazł xD\n",
    "\n",
    "Jest w tym papierze fajny przyjład liczenia i-vectorów z HMM. Podobny do moich wyprowadzeń wczesniejszych. Mogę porównać z moimi, jest spoko. \n",
    "\n",
    "![reddots-results0-hmmivec](images/reddots-results0-hmmivec.png)\n",
    "![reddots-results1-hmmivec](images/reddots-results1-hmmivec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random - Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WAVENET: A GENERATIVE MODEL FOR RAW AUDIOA\n",
    "### https://arxiv.org/pdf/1609.03499.pdf\n",
    "\n",
    "They used NN to generate audio. Their network was convolutional and given past samples generated the next one, which could then be prepended to input for the next sample.\n",
    "\n",
    "The used dilated convolution, that is a convolution that skips input values with a certain step. It increased receptive field to $2^{\\text{depth}}$\n",
    "\n",
    "They used softmax, but quantized $2^{16} = 65536$ levels to $256$. The used nonlinear quantization \n",
    "$$f(x_t) = sign(x_t) \\frac{ln(1 + \\mu|x_t|}{ln(1 + \\mu)}$$\n",
    "where $-1 \\lt x_t \\lt 1$ and $\\mu = 255$\n",
    "\n",
    "They used gated activation units like in _PixelCNN_.\n",
    "$$z = tanh(W_{f,k} * x) \\cdot \\sigma(W_{g,k} * x)$$\n",
    "where $*$ is convolution operator, $\\cdot$ is element-wise multiplication, $\\sigma$ is a sigmoid function, $k$ layer index, $f$ and $g$ are filter and gate, $W$ is learnable convolution filter. It worked better than ReLUs.\n",
    "\n",
    "You can make the model generate speech with some characteristics by conditioning it on it, eg. on speaker identity as one hot vector.\n",
    "\n",
    "Some text-depended models also exists, they use external models for predicting $log F_0$ and phone duration from linguistic features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fader Networks:Manipulating Images by Sliding Attributes\n",
    "### https://arxiv.org/pdf/1706.00409.pdf\n",
    "\n",
    "The paper is about a generative image model. The model learned features of human faces like whether the person wears glasses, whether they're old or young or what is their sex. It could generate given a face the same face but with some of those features changed.\n",
    "\n",
    "They used encoder - decoder architecture. Encoder is a convolutional NN $D_{\\gamma_enc}: X \\to R^N$, decoder is a deconvolutional NN $D_{\\gamma_dec}: (R^N, y) \\to X$. Autoencoder loss is a MSE between image and its reconstruction. \n",
    "\n",
    "They had a problem learning the network, because it ignored y. They obviated the problem by forcing the model to have same $E(X)$ for all images $X$ of the same person, but with selected features $y$ changing. They used adversarial training on the latent space. The idea is to learn an additional NN called discriminator that tries to predict y given E(X) and ensuring that it is unable to identify y. This corresponds to a two-player where discriminator tries to learn y and E tries to prevent it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Speech: Scaling up end-to-endspeech recognition\n",
    "### https://arxiv.org/pdf/1412.5567.pdf\n",
    "\n",
    "3 Nonrecursive -> 1 LSTM -> 1 Nonrecursive\n",
    "\n",
    "Trained on\n",
    "\n",
    "| Dataset     | Type           | Hours | Speakers |\n",
    "|:----------- |:-------------- | -----:| --------:|\n",
    "| WSJ         | read           | 80    | 280      |\n",
    "| Switchboard | conversational | 300   | 4000     | \n",
    "| Fisher      | conversational | 2000  | 23000    |\n",
    "| Baidu       | read           | 5000  | 9600     |\n",
    "\n",
    "(the model available at github I think didn't include Baidu 5000 hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization: Accelerating Deep Network Training byReducing Internal Covariate Shift\n",
    "### https://arxiv.org/pdf/1502.03167.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Is All You Need\n",
    "### https://arxiv.org/pdf/1706.03762.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
