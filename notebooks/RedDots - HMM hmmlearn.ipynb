{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM GMM\n",
    "\n",
    "We use 3-state mono-phone HMMs to construct this recognizer. The emission probability of every state is modeled by a GMM. Say we have F mono-phones (F = 39 in our English lexicon), and a G-mixture GMM for each mono-phone state. Thus the GMM-HMM has 3FG mixture components in total. Compared to GMM-UBM, these mixtures are better separated in the phoneme space.\n",
    "\n",
    "MFCC:  MFCCs  are  extracted  from  16kHz  utterance  with  40 filter-banks distributed between 0 and 8kHz. Static 19-dimensional coefficients plus energy and their delta and delta-delta form a 60-dimensional vector. CMVN is applied per utterance.\n",
    "\n",
    "Hmm, deepspeech ma 26 MFCC features i zdaje się, że usuwa co drugi, ale to w związku z jakąś sztuczką uczenia RNN. Mają tak: features = mfcc(audio, samplerate=fs, numcep=numcep) gdzie mfcc jest z psf jak u nas xD a numcep=26 domyślnie, a fs=16000 domyślnie\n",
    "\n",
    "Hmm, inny papier: In our HMM-based method, a phoneme recognizer is firsttrained with 3-state, GMM-based, mono-phone HMMs. Thisrecognizer is the same as in speech recognition. LetFbe thetotal number of mono-phones (i.e. 39),S=3Fbe the numberof all states,Gthe number of Gaussian components per state,andC=SGthe number of all individual Gaussians, and let(s, g)denote Gaussian componentgin states.\n",
    "3-state - what are you?\n",
    "\n",
    "Given a transcription, a graph of HMM is composed.\n",
    "\n",
    "The  Viterbi  and  forward-backward  (FB)  algorithms  are  twomeans to align frames to states and mixtures.\n",
    "\n",
    "Speaker adaption is the same with Eq. 4 except mixtures here are phonetic dependent.\n",
    "\n",
    "During the test phase,  the Viterbi-based log-likelihood ratio is expressed as:\n",
    "sum_t log P(x_t | model_user,qt) - log P(x_t | model_ubm,qt)\n",
    "\n",
    "HMM: To generate the alignment for the HMM-based modeling,we use MFCCs to train the HMM. 39 mono-phones plus a silencemodel are used,  each of which contains 3 states.  To model thecomplexity of silence, a GMM with 16 mixtures is used for every silence state, while other states are all modeled by 8 Gaussians,resulting 984 Gaussians in total. This HMM is further extended toa triphone system and remains 2142 senones.  The transcriptionsfor  DNN  training  is  generated  by  the  senone  alignment.   OnlyMFCCs are used for HMM training and alignment.\n",
    "\n",
    "GMM-HMM and i-vector/HMM: The GMM of every state is re-estimated using the HMM alignments and different speaker fea-tures.   The total number of mixtures in our model is 984.   Thedimension of i-vector is again set to 600.  Viterbi and FB align-ments are both investigated\n",
    "\n",
    "* 39 x 3 states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP - Książka Rabinera:\n",
    "\n",
    "MAP adaptation of speaker-independent model to a speaker of parameters is obtained by solving \n",
    "\n",
    "$$\\frac{d}{d\\lambda} P(\\lambda | O) = 0$$\n",
    "\n",
    "where\n",
    "\n",
    "$$P(\\lambda | O) = \\frac{P(O | \\lambda)P(\\lambda)}{P(O)}$$\n",
    "\n",
    "If we have a good prior estimated from a lot of data, then we can use it to get a model for a speaker for which data is scarce.\n",
    "\n",
    "Conjugate priors for a random vector is defined as the prior distribution for the parameters of the parameters of the probaiblity density function of the random vector, such that the posterior distribution $P(\\lambda | O)$ and the prior distribution $P(\\lambda)$ belong to the same distribution family for any sample observations $O$.\n",
    "For example, it is well known that the conjugate prior for the mean of a Gaussian density is also a Gaussian density.\n",
    "\n",
    "$$\\bar{u_{MAP}} = \\frac{n \\tau^2}{\\sigma^2 + n \\tau^2} \\bar{o} + \\frac{\\sigma^2}{\\sigma^2 + n \\tau^2} \\rho$$\n",
    "\n",
    "where $\\bar{o}$ - sample mean of new data, $n$ - number of training observations, $\\tau^2$ - variance of prior, $\\rho$ - mean of prior distribution\n",
    "\n",
    "How do we estimate $\\rho$ and $\\tau^2$ of prior? From a collection of speaker-dependent models (and $c_m$ is a weight of a model) or from a speaker-independent model with mixture of distributions in each state ($c_m$ is a weight of a mixture component)\n",
    "\n",
    "$\\rho = \\sum_{m=1}^M c_m \\rho_m$\n",
    "\n",
    "$\\tau^2 = \\sum_{m=1}^M c_m (\\rho_m - \\rho)$\n",
    "\n",
    "System do rozpoznawania słowa:\n",
    "\n",
    "po jednym modelu na słowo, rozpoznając matchujemy z każdym po kolei i bierzemy max prob\n",
    "model left-to-right zamiast ergodic :OOOO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://web.stanford.edu/class/cs224s/lectures/\n",
    "\n",
    "Hmm, Viterbi learning, hard assignment: Bahm ~ o3 \n",
    "EM learning, soft assignment: Bahm ~ 0*o1 + 0.15*o2 + 0.5*o3 + 0.05*o\n",
    "\n",
    "Typical training procedure in LVCSR\n",
    "Generate a forced alignment with existing model\n",
    "Viterbi decoding with a very constrained prior (the transcript)\n",
    "Assigns observations to HMM states\n",
    "Create new observation models from update alignments\n",
    "Iteratively repeat the above steps, occasionally introducing a more complex observation model or adding more difficult training examples\n",
    "\n",
    "Viterbi Beam search\n",
    "\n",
    "Applying FB to speech: Caveats\n",
    "Network structure of HMM is always created by hand\n",
    "no algorithm for double-induction of optimal structure and probabilities has been able to beat simple hand-built structures.\n",
    "Always Bakisnetwork = links go forward in time\n",
    "Subcase of Bakisnet: beads-on-string net\n",
    "\n",
    "Assume that the features in the feature vector are uncorrelated\n",
    "This isn’t true for FFT features, but is true for MFCC features, as we will see later\n",
    "\n",
    "\n",
    "Initialization: “Flat start”Transition probabilities:set to zero any that you want to be “structurally zero”Set the rest to identical valuesLikelihoods:initialize μand sof each state to global mean and variance of all training data\n",
    "\n",
    "Embedded TrainingGiven: phoneset, lexicon, transcribed wavefilesBuild a whole sentence HMM for each sentenceInitialize A probsto 0.5, or to zeroInitialize B probsto global mean and varianceRun multiple iterations of Baum WelchDuring each iteration, we compute forward and backward probabilitiesUse them to re-estimate A and BRun Baum-Welch until convergence\n",
    "\n",
    "Viterbi training rather than Baum-Welch training. Computing the “Viterbi path” over the training data is called “forced alignment”\n",
    "Because we know which word string to assign to each observation sequence.\n",
    "We just don’t know the state sequence.\n",
    "So we use aijto constrain the path to go through the correct words\n",
    "\n",
    "Modeling phonetic context\n",
    "The strongest factor affecting phonetic variability is the neighboring phone\n",
    "Idea: have phone models which are specific to context\n",
    "Instead of Context-Independent (CI) phonesWe’ll have Context-Dependent (CD) phones\n",
    "\n",
    "\n",
    "Speech signal is not constantslope of formants, change from stop burst to releaseSo in addition to the cepstralfeaturesNeed to model changes in the cepstralfeatures over time.“delta features”“double delta” (acceleration) features\n",
    "\n",
    "Typical MFCC featuresWindow size: 25msWindow shift: 10msPre-emphasis coefficient: 0.97MFCC:12 MFCC (mel frequency cepstral coefficients)1 energy feature12 delta MFCC features 12 double-delta MFCC features1 delta energy feature1 double-delta energy featureTotal 39-dimensional features\n",
    "\n",
    "\n",
    "Acoustic Model AdaptationShift the means and variances of Gaussians to better match the input feature distributionMaximum Likelihood Linear Regression (MLLR)Maximum A Posteriori (MAP) AdaptationFor both speaker adaptation and environment adaptation\n",
    "\n",
    "MAP Adaptation: u_new = N / (N + a) u_estimated + a / (N + a) u_old, where a is some weight of old\n",
    "\n",
    "DUŻO BYŁO W lec5\n",
    "\n",
    "SIL is a phoneme to a recognizerAlways inserted at start and end of utteranceCorrupting silence with bad forced alignments can break recognizer training (silence eats everything)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hmm, no dobra, plan działania:\n",
    "\n",
    "Preprocessing:\n",
    "wytnij ciszę z dźwięków, voice activity detection \n",
    "Wylicz MFCC, 20 stanów + d + dd \n",
    "\n",
    "Training:\n",
    "Dla każdego sentence_id (jest ich 10):\n",
    "    Utwórz HMMGMM z całego zbioru treningowego sequence_id:\n",
    "        HMM with 3 states and 8 Gaussian components for each of 39 mono-phones were used for the alignment (resulting in to-tal number of 936 Gaussian components) ?? \n",
    "        albo unique(znaki) * 3 stany\n",
    "        albo znaki * 3 stany trójkątna! (Bakis)\n",
    "        \n",
    "Enrollment:\n",
    "Dla każdego gspeaker_id:\n",
    "    Dla każdego sentence_id:\n",
    "        Weź dane (gspeaker_id, sequence_id)\n",
    "        Naucz HMM taki jak wyżej tylko na nich\n",
    "        Dokonaj MAP adaptacji\n",
    "        \n",
    "Trial:\n",
    "likelihood HMM_{podany speaker, podane zdanie} - likelihood HMM_{UBM, podane zdanie} ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import concurrent.futures as cf\n",
    "import functools as ft\n",
    "import itertools as it\n",
    "import json\n",
    "import math\n",
    "import operator as op\n",
    "import os\n",
    "\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interact_manual, widgets\n",
    "from hmmlearn import hmm\n",
    "from hmmlearn import base as hmmbase\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import interpolate, misc, optimize, spatial, stats\n",
    "from sklearn import metrics, mixture, cluster, utils\n",
    "\n",
    "from paprotka.dataset import reddots\n",
    "from paprotka.feature import cepstral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('once')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = reddots.get_root()\n",
    "load_pcm = ft.partial(reddots.load_pcm, root)\n",
    "load_mfcc = ft.partial(reddots.load_npy, root, 'wac2_mfcc13_ddd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enrollments\n",
      "is_male                  bool\n",
      "pcm_path               object\n",
      "sentence_id             int16\n",
      "speaker_id              int16\n",
      "timestamp      datetime64[ns]\n",
      "dtype: object\n",
      "Trials\n",
      "correct_sentence                  bool\n",
      "expected_is_male                  bool\n",
      "expected_sentence_id             int16\n",
      "expected_speaker_id              int16\n",
      "pcm_path                        object\n",
      "target_person                     bool\n",
      "trial_is_male                     bool\n",
      "trial_sentence_id                int16\n",
      "trial_speaker_id                 int16\n",
      "trial_timestamp         datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "enrollments_1 = reddots.load_enrollments(root + '/ndx/f_part_01.trn', root + '/ndx/m_part_01.trn')\n",
    "print('Enrollments', enrollments_1.dtypes, sep='\\n')\n",
    "\n",
    "trials_1 = reddots.load_trials(root + '/ndx/f_part_01.ndx', root + '/ndx/m_part_01.ndx')\n",
    "print('Trials', trials_1.dtypes, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      sentence_id                  content  \\\n",
      "1609           31  My voice is my password   \n",
      "\n",
      "                                                 phones  \n",
      "1609  M AY1 _ V OY1 S _ IH1 Z _ M AY1 _ P AE1 S W ER2 D  \n"
     ]
    }
   ],
   "source": [
    "# script = reddots.load_script(root + '/infos/script.txt')\n",
    "script = pd.read_csv(root + '/infos/phones.csv', sep=';')\n",
    "print(script.loc[script['sentence_id'] == 31])\n",
    "\n",
    "enrollments_1 = enrollments_1.merge(script, how='left', on='sentence_id')\n",
    "trials_1 = trials_1.merge(script, how='left', left_on='expected_sentence_id', right_on='sentence_id')\n",
    "del trials_1['sentence_id']\n",
    "trials_1.rename(columns={'content': 'expected_content', 'phones': 'expected_phones'}, inplace=True)\n",
    "trials_1 = trials_1.merge(script, how='left', left_on='trial_sentence_id', right_on='sentence_id')\n",
    "del trials_1['sentence_id']\n",
    "trials_1.rename(columns={'content': 'trial_content', 'phones': 'trial_phones'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = enrollments_1[enrollments_1.sentence_id == 31]\n",
    "example_features = [load_mfcc(path) for path in example['pcm_path']]\n",
    "example_lengths = [mfcc.shape[0] for mfcc in example_features] \n",
    "example_features_stack = np.vstack(example_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M', 'AY1', '_', 'V', 'OY1', 'S', '_', 'IH1', 'Z', '_', 'M', 'AY1', '_', 'P', 'AE1', 'S', 'W', 'ER2', 'D']\n",
      "['_', 'M', 'AY', '_', 'V', 'OY', 'S', '_', 'IH', 'Z', '_', 'M', 'AY', '_', 'P', 'AE', 'S', 'W', 'ER', 'D', '_']\n",
      "['AE' 'AY' 'D' 'ER' 'IH' 'M' 'OY' 'P' 'S' 'V' 'W' 'Z' '_']\n"
     ]
    }
   ],
   "source": [
    "def wrap_with_silence(phones):\n",
    "    return ['_'] + phones + ['_']\n",
    "\n",
    "def remove_stress_combinations(phones):\n",
    "    def remove_single_phone(phone):\n",
    "        if phone[-1].isdigit():\n",
    "            return phone[:-1]\n",
    "        else:\n",
    "            return phone\n",
    "        \n",
    "    return [remove_single_phone(phone) for phone in phones]\n",
    "\n",
    "example_phones = example.phones.loc[0].split(' ')\n",
    "example_phones_cleaned = wrap_with_silence(remove_stress_combinations(example_phones))\n",
    "# example_phones_cleaned = remove_stress_combinations(example_phones)\n",
    "example_phones_unique = np.unique(example_phones_cleaned)\n",
    "\n",
    "print(example_phones)\n",
    "print(example_phones_cleaned)\n",
    "print(example_phones_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "## unique\n",
    "n_components = 3 * example_phones_unique.size\n",
    "example_classifier_unique = hmm.GMMHMM(n_components=n_components, n_mix=8)\n",
    "example_classifier_unique.fit(example_features_stack, example_lengths);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianMixture(covariance_type='diag', init_params='kmeans', max_iter=100,\n",
       "        means_init=None, n_components=8, n_init=1, precisions_init=None,\n",
       "        random_state=None, reg_covar=1e-06, tol=0.001, verbose=0,\n",
       "        verbose_interval=10, warm_start=False, weights_init=None)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg_model = mixture.GaussianMixture(n_components=8, covariance_type='diag')\n",
    "bg_model.fit(example_features_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([[ 0.2       ,  0.2       ,  0.        ,  0.        ,  0.2       ,\n",
      "         0.        ,  0.        ,  0.2       ,  0.2       ],\n",
      "       [ 0.        ,  0.5       ,  0.5       ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.5       ,  0.5       ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.25      ,  0.25      ,\n",
      "         0.        ,  0.        ,  0.25      ,  0.25      ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.5       ,\n",
      "         0.5       ,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.5       ,  0.5       ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.33333333,  0.33333333,  0.33333333],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.5       ,  0.5       ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  1.        ]]))\n"
     ]
    }
   ],
   "source": [
    "def make_matrix_beads(size):\n",
    "    initial = np.zeros(size)\n",
    "    initial[0] = 1.0\n",
    "    transitions = np.eye(size)\n",
    "    transitions[:, 1:] += np.eye(size, size - 1)\n",
    "#     transitions[-1, -2] = 1\n",
    "    transitions /= transitions.sum(axis=1)[:, np.newaxis]\n",
    "    return initial, transitions\n",
    "\n",
    "def make_matrix_triangular_beads(size, phones, phone_size=3, silence_size=1):\n",
    "    initial = np.zeros(size)\n",
    "    initial[0] = 1.0\n",
    "    \n",
    "    next_ix = 0\n",
    "    starts = np.zeros(size)\n",
    "    for phone in phones:\n",
    "        starts[next_ix] = 1\n",
    "        if phone == '_':\n",
    "            next_ix += silence_size\n",
    "        else:\n",
    "            next_ix += phone_size\n",
    "    ends = starts[1:]\n",
    "    \n",
    "    transitions = np.eye(size)\n",
    "    transitions[:, 1:] += np.eye(size, size - 1)\n",
    "#     transitions[-1, -2] = 1\n",
    "\n",
    "    for end_ix in np.where(ends)[0]:\n",
    "        transitions[end_ix, end_ix+1:] = starts[end_ix+1:]\n",
    "\n",
    "    transitions /= transitions.sum(axis=1)[:, np.newaxis]\n",
    "    return initial, transitions\n",
    "\n",
    "make_matrix_beads(7)\n",
    "print(make_matrix_triangular_beads(9, ['_', 'A', 'B', '_', '_']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.17406918  0.07971898  0.06388068  0.2542175   0.1666166   0.06764632\n",
      "  0.06812612  0.12572463]\n",
      "[[  1.12804076e+01  -1.43035635e+01  -1.33024575e-01  -9.46662018e-01\n",
      "    1.42936349e+00   1.02261308e+00  -3.38650901e+00  -6.84189738e-01\n",
      "   -2.51636292e+00   6.76333596e-01  -2.08126090e+00   1.04897852e+00\n",
      "   -1.28166446e+00  -2.42064069e-02  -8.01072658e-02   6.08988313e-02\n",
      "    9.19454598e-03   4.97571247e-02   4.80897194e-02   4.50676825e-02\n",
      "    8.28164009e-03  -6.21901271e-02  -6.68429365e-02   6.42958377e-03\n",
      "    1.35040588e-02  -1.78331209e-02   9.72259057e-03   2.09559059e-02\n",
      "    4.71504369e-03   1.80189763e-02  -4.71426935e-03  -6.25000137e-03\n",
      "   -8.06028949e-03  -2.43505710e-02  -2.17094405e-02  -7.91273859e-03\n",
      "   -1.62002664e-03  -8.51477843e-03  -1.63135388e-02]\n",
      " [  1.82575007e+01  -3.39841582e+01   1.55370353e+01  -7.54255772e+00\n",
      "   -3.62655996e+00   1.59190706e+00  -1.33740901e+01   7.06393712e-01\n",
      "   -2.76356502e+00  -9.35160035e-01  -1.61003912e+00  -1.58955059e+00\n",
      "   -1.73495409e+00  -1.12820703e-01  -4.87538492e-01  -3.50341803e-01\n",
      "    8.24705745e-02   1.83318432e-02   4.81462169e-01   7.61418713e-01\n",
      "   -3.26977868e-01   9.35967911e-04   2.69940044e-01  -8.60383483e-02\n",
      "    1.98267487e-01   2.05987699e-01  -9.06374754e-02   9.01720864e-01\n",
      "   -3.87213039e-01   2.12141307e-01  -3.86214946e-02  -4.00883928e-01\n",
      "   -5.70504823e-02  -2.53306241e-01  -2.35619363e-01  -7.43157623e-02\n",
      "   -3.55991330e-02   4.76293359e-03  -1.97108523e-01]]\n",
      "[[  2.86908855e+00   6.87000769e+01   7.87978775e+01   6.58857380e+01\n",
      "    8.06897396e+01   9.31405867e+01   1.01194547e+02   8.20076595e+01\n",
      "    9.83350228e+01   7.82563619e+01   6.95419470e+01   5.35038878e+01\n",
      "    5.27184065e+01   1.65554038e-02   4.01401344e-01   6.57856868e-01\n",
      "    7.45851236e-01   8.99928857e-01   1.06021722e+00   1.12816105e+00\n",
      "    1.17728186e+00   1.22638003e+00   1.29783377e+00   1.14659106e+00\n",
      "    1.06368291e+00   9.74035149e-01   1.10378830e-03   2.67738589e-02\n",
      "    3.92154978e-02   4.54672791e-02   5.20795760e-02   5.85111202e-02\n",
      "    6.60114806e-02   6.92981059e-02   7.43718233e-02   7.87500505e-02\n",
      "    6.78118486e-02   6.32381623e-02   5.99930324e-02]\n",
      " [  3.99554055e+00   6.09960724e+01   1.57309273e+02   1.67164013e+02\n",
      "    2.32702392e+02   1.77333789e+02   2.28261336e+02   1.80773176e+02\n",
      "    1.52106458e+02   1.08779099e+02   8.71319528e+01   9.22630434e+01\n",
      "    7.54118248e+01   1.96544737e-01   1.04680526e+01   4.44258947e+00\n",
      "    4.54358298e+00   2.72541388e+00   5.16140005e+00   4.31749204e+00\n",
      "    4.13229003e+00   3.58383650e+00   2.82186285e+00   2.64700978e+00\n",
      "    2.36897524e+00   2.35135760e+00   4.80126817e-03   2.12426762e-01\n",
      "    2.92474226e-01   2.44418823e-01   2.31161900e-01   2.37408827e-01\n",
      "    2.77993612e-01   2.44196457e-01   2.35367777e-01   1.88285863e-01\n",
      "    1.65476762e-01   1.59550045e-01   1.16160153e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(bg_model.weights_)\n",
    "print(bg_model.means_[0:2])\n",
    "print(bg_model.covariances_[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GMMHMM(algorithm='viterbi', covariance_type='diag',\n",
       "    covars_prior=array([[[-1.5, -1.5, ..., -1.5, -1.5],\n",
       "        [-1.5, -1.5, ..., -1.5, -1.5],\n",
       "        ...,\n",
       "        [-1.5, -1.5, ..., -1.5, -1.5],\n",
       "        [-1.5, -1.5, ..., -1.5, -1.5]],\n",
       "\n",
       "       [[-1.5, -1.5, ..., -1.5, -1.5],\n",
       "        [-1.5, -1.5, ..., -1.5, -1.5],\n",
       "        ...,\n",
       "        [-1.5, -1.5, .....-1.5],\n",
       "        ...,\n",
       "        [-1.5, -1.5, ..., -1.5, -1.5],\n",
       "        [-1.5, -1.5, ..., -1.5, -1.5]]]),\n",
       "    covars_weight=array([[[ 0.,  0., ...,  0.,  0.],\n",
       "        [ 0.,  0., ...,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0., ...,  0.,  0.],\n",
       "        [ 0.,  0., ...,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0., ...,  0.,  0.],\n",
       "        [ 0.,  0., ...,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0., ...,  0.,  0.],\n",
       "        [ 0.....,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0., ...,  0.,  0.],\n",
       "        [ 0.,  0., ...,  0.,  0.]]]),\n",
       "    init_params='',\n",
       "    means_prior=array([[[ 0.,  0., ...,  0.,  0.],\n",
       "        [ 0.,  0., ...,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0., ...,  0.,  0.],\n",
       "        [ 0.,  0., ...,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0., ...,  0.,  0.],\n",
       "        [ 0.,  0., ...,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0., ...,  0.,  0.],\n",
       "        [ 0.,.....,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0., ...,  0.,  0.],\n",
       "        [ 0.,  0., ...,  0.,  0.]]]),\n",
       "    means_weight=array([[ 0.,  0., ...,  0.,  0.],\n",
       "       [ 0.,  0., ...,  0.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0., ...,  0.,  0.],\n",
       "       [ 0.,  0., ...,  0.,  0.]]),\n",
       "    min_covar=0.001, n_components=51, n_iter=10, n_mix=8, params='tmcw',\n",
       "    random_state=None, startprob_prior=1.0, tol=0.01, transmat_prior=1.0,\n",
       "    verbose=False,\n",
       "    weights_prior=array([[ 1.,  1., ...,  1.,  1.],\n",
       "       [ 1.,  1., ...,  1.,  1.],\n",
       "       ...,\n",
       "       [ 1.,  1., ...,  1.,  1.],\n",
       "       [ 1.,  1., ...,  1.,  1.]]))"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## beads\n",
    "# params=subset of 'stmcw'\n",
    "def calculate_n_components(phones, phone_n=3, silence_n=1):\n",
    "    n_components = phone_n * len(phones)\n",
    "    \n",
    "    def silence_indices(phone):\n",
    "        return [True] * silence_n if phone == '_' else [False] * phone_n\n",
    "    silences = np.array([v for phone in phones for v in silence_indices(phone) ], dtype=np.bool)\n",
    "    \n",
    "    if phone_n != silence_n:\n",
    "        difference = silence_n - phone_n\n",
    "        n_components += silences.sum() * difference\n",
    "    return n_components, silences\n",
    "\n",
    "def make_classifier_beads(phones, bg_model):\n",
    "    n_components, silences = calculate_n_components(phones)\n",
    "#     initial, transitions = make_matrix_triangular_beads(n_components, phones)\n",
    "    initial, transitions = make_matrix_beads(n_components)\n",
    "\n",
    "    classifier = hmm.GMMHMM(\n",
    "        n_components=n_components, n_mix=8, covariance_type='diag', init_params='', params='tmcw'\n",
    "    )\n",
    "\n",
    "    classifier.startprob_ = initial\n",
    "    classifier.transmat_ = transitions\n",
    "    classifier.weights_ = np.tile(bg_model.weights_, (n_components, 1))\n",
    "    means = np.tile(bg_model.means_, (n_components, 1, 1))\n",
    "    means += np.random.normal(scale=0.1, size=means.shape)\n",
    "    classifier.means_ = means\n",
    "    covars = np.tile(bg_model.covariances_, (n_components, 1, 1))\n",
    "#     covars += np.abs(np.random.normal(scale=0.01, size=covars.shape))\n",
    "    classifier.covars_ = covars \n",
    "#     classifier.covars_[silences] *= 5\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "example_classifier_beads = make_classifier_beads(example_phones_cleaned, bg_model)\n",
    "example_classifier_beads.fit(example_features_stack, example_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-52967.77978585746, array([ 0,  1,  1,  1,  1,  1,  1,  2,  3,  4,  5,  6,  7,  8,  8,  8,  8,\n",
      "        8,  8,  8,  9, 10, 11, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "       14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16]))\n",
      "(-43574.82028038213, array([ 0,  1,  2,  3,  4,  5,  6,  6,  6,  6,  6,  7,  8,  9, 10, 11, 11,\n",
      "       11, 11, 11, 11, 11, 12, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14,\n",
      "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "       15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16]))\n",
      "(-44709.77131001235, array([ 0,  1,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 12, 12, 13,\n",
      "       13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "       14, 14, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16]))\n"
     ]
    }
   ],
   "source": [
    "for i in (0, 50, 100):\n",
    "    print(example_classifier_beads.decode(example_features[i], algorithm='viterbi'))\n",
    "#     print(example_classifier_beads.decode(example_features[i], algorithm='map'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means\n",
      "[[[ 16.25947214   0.32716725  18.09623335   3.99491784]\n",
      "  [ 16.25947213   0.32716731  18.0962334    3.99491783]\n",
      "  [ 16.25947214   0.32716722  18.09623333   3.99491784]\n",
      "  [ 16.25947214   0.32716725  18.09623335   3.99491784]]\n",
      "\n",
      " [[ 12.82098929 -14.92699264  24.71795361   6.32869305]\n",
      "  [ 12.82098929 -14.92699264  24.71795361   6.32869305]\n",
      "  [ 12.82098929 -14.92699264  24.71795361   6.32869305]\n",
      "  [ 12.82098929 -14.92699264  24.71795361   6.32869305]]\n",
      "\n",
      " [[ 12.50236988 -22.27513711  14.89559182  -2.48796384]\n",
      "  [ 12.50236988 -22.27513711  14.89559182  -2.48796384]\n",
      "  [ 12.50236988 -22.27513711  14.89559182  -2.48796384]\n",
      "  [ 12.50236988 -22.27513711  14.89559182  -2.48796384]]\n",
      "\n",
      " [[ 12.63585093 -28.44920449   9.11174904  -9.34496307]\n",
      "  [ 12.63585093 -28.44920449   9.11174904  -9.34496307]\n",
      "  [ 12.63585093 -28.44920449   9.11174904  -9.34496307]\n",
      "  [ 12.63585093 -28.44920449   9.11174904  -9.34496307]]]\n",
      "[[[ 12.96333628 -21.60978125  17.38953096  -6.44232739]\n",
      "  [ 12.9633362  -21.60980235  17.38953274  -6.44229465]\n",
      "  [ 12.96333619 -21.60980368  17.38953279  -6.44229237]\n",
      "  [ 12.96333624 -21.60979835  17.38953251  -6.44229994]]\n",
      "\n",
      " [[ 12.77805675 -23.73624809  15.91622554  -7.87143014]\n",
      "  [ 12.77805675 -23.73624809  15.91622554  -7.87143014]\n",
      "  [ 12.77805675 -23.73624809  15.91622554  -7.87143014]\n",
      "  [ 12.77805675 -23.73624809  15.91622554  -7.87143014]]]\n",
      "Covars\n",
      "[[[  6.47543241e+00   4.14720219e+01   1.98839580e+02   1.14988485e+00]\n",
      "  [  6.47543238e+00   4.14720222e+01   1.98839580e+02   1.14988484e+00]\n",
      "  [  6.47543243e+00   4.14720217e+01   1.98839581e+02   1.14988485e+00]\n",
      "  [  6.47543242e+00   4.14720218e+01   1.98839580e+02   1.14988485e+00]]\n",
      "\n",
      " [[  5.90691485e-02   3.81292854e+00   2.30992251e+01   3.21884361e+01]\n",
      "  [  5.90691486e-02   3.81292854e+00   2.30992251e+01   3.21884361e+01]\n",
      "  [  5.90691486e-02   3.81292854e+00   2.30992251e+01   3.21884361e+01]\n",
      "  [  5.90691486e-02   3.81292854e+00   2.30992251e+01   3.21884361e+01]]\n",
      "\n",
      " [[  1.84588552e-02   3.00447910e+00   6.81312168e+00   5.96748720e+00]\n",
      "  [  1.84588552e-02   3.00447910e+00   6.81312168e+00   5.96748720e+00]\n",
      "  [  1.84588552e-02   3.00447910e+00   6.81312168e+00   5.96748720e+00]\n",
      "  [  1.84588552e-02   3.00447910e+00   6.81312168e+00   5.96748720e+00]]\n",
      "\n",
      " [[  8.75322119e-03   3.74184215e+00   2.20963736e+00   3.04854658e+00]\n",
      "  [  8.75322119e-03   3.74184215e+00   2.20963736e+00   3.04854658e+00]\n",
      "  [  8.75322119e-03   3.74184215e+00   2.20963736e+00   3.04854658e+00]\n",
      "  [  8.75322118e-03   3.74184215e+00   2.20963736e+00   3.04854658e+00]]]\n",
      "[ 0.41029974  0.50355818  0.08207258  0.12439239  0.14535458  0.10756906\n",
      "  0.98421737  0.82317554  0.87693316  0.97887919  0.96980538  0.76607721\n",
      "  0.75014155  0.71041225  0.92817872  0.71827348  0.7765157   0.54249103\n",
      "  0.52258974  0.53842794  0.92899918  0.62446856  0.80609853  0.76827279\n",
      "  0.55922237  0.39190174  0.37814742  0.59675683  0.6228757   0.8710917\n",
      "  0.91370455  0.7448976   0.73795169  0.70275267  0.61158128  0.59659541\n",
      "  0.59899661  0.88610145  0.24059038  0.21696621  0.73166498  0.35502067\n",
      "  0.43416756  0.39118306  0.9687853   0.96046396  0.16867761  0.33380709\n",
      "  0.67871592  0.6626257   1.        ]\n",
      "['_', 'M', 'AY', '_', 'V', 'OY', 'S', '_', 'IH', 'Z', '_', 'M', 'AY', '_', 'P', 'AE', 'S', 'W', 'ER', 'D', '_']\n"
     ]
    }
   ],
   "source": [
    "print('Means')\n",
    "print(example_classifier_beads.means_[0:4, 0:4, 0:4])\n",
    "print(example_classifier_beads.means_[-2:, 0:4, 0:4])\n",
    "print('Covars')\n",
    "print(example_classifier_beads.covars_[0:4, 0:4, 0:4])\n",
    "# np.tile(bg_model.means_, (n_components, 1, 1))[0:4, 0:4, 0:4]\n",
    "# example_classifier_beads.means_.shape\n",
    "print(example_classifier_beads.transmat_.diagonal())\n",
    "print(example_phones_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_training(classifier, enrollments):\n",
    "    labels = enrollments[['is_male', 'speaker_id', 'sentence_id']].values\n",
    "    features = [load_mfcc(path) for path in enrollments_1['pcm_path']]\n",
    "    classifier.fit(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GMMHMM(algorithm='viterbi', covariance_type='diag', covars_prior=0.01,\n",
       "    init_params='stmcw', n_components=39, n_iter=10, n_mix=8,\n",
       "    params='stmcw', random_state=None, startprob_prior=1.0, tol=0.01,\n",
       "    transmat_prior=1.0, verbose=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_speaker_classifier(inspeaker_classifier):\n",
    "    n_components = 3 * len(phones)\n",
    "    initial, transitions = make_matrix_beads(n_components)\n",
    "\n",
    "    classifier = hmm.GMMHMM(\n",
    "        n_components=n_components, n_mix=8, covariance_type='diag', init_params='mcw', params='tmcw'\n",
    "    )\n",
    "\n",
    "    classifier.startprob_ = initial\n",
    "    classifier.transmat_ = transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_enrollments(classifier, enrollments):\n",
    "    labels = enrollments[['is_male', 'speaker_id', 'sentence_id']].values\n",
    "    features = [load_mfcc(path) for path in enrollments_1['pcm_path']]\n",
    "    classifier.fit(features, labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(label, results):\n",
    "    path = os.path.join(root, 'result', label)\n",
    "    with open(path) as opened:\n",
    "        pickle.dump(results, opened)\n",
    "        \n",
    "def load_results(label):\n",
    "    path = os.path.join(root, 'result', label)\n",
    "    with open(path) as opened:\n",
    "        return pickle.load(opened)\n",
    "    \n",
    "def perform_trial(classifier, path):\n",
    "    features = load_mfcc(path)\n",
    "    return classifier.predict_single_proba(features)\n",
    "\n",
    "def perform_trials(classifier, trials):\n",
    "    paths = trials['pcm_path'].unique()\n",
    "    results = {}\n",
    "    for path in paths:\n",
    "        results[path] = perform_trial(classifier, path)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_phone_weights = {}\n",
    "\n",
    "per_phone_weights.get('A', np.eye(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bakis:\n",
    "    def __init__(self, phones, per_phone_weights, ignore_stress=True, wrap_silence=True):\n",
    "        self.phone_seq = phones.split(' ')\n",
    "        if wrap_silence:\n",
    "            self.phone_seq = ['_'] + self.phone_seq + ['_']\n",
    "            \n",
    "        state_transitions = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31, 32, 33, 34, 35, 36, 37, 38, 39, 40])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enrollments_1.sentence_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_enrollments = enrollments_1.loc[enrollments_1.sentence_id == 31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HmmGmmVerifier:\n",
    "    def __init__(self):\n",
    "        self.patterns = None\n",
    "        self.labels = None\n",
    "\n",
    "    def fit(self, features, labels):\n",
    "        self.patterns = features\n",
    "        self.labels = labels\n",
    "        self.unique_labels = np.unique(labels)\n",
    "        \n",
    "    def enroll(self, ):\n",
    "        pass\n",
    "    \n",
    "    def trial(self, ):\n",
    "        pass\n",
    "\n",
    "    def predict(self, features, metric=spatial.distance.cosine):\n",
    "        sequence_label_proba = self.predict_proba(features, metric)\n",
    "        max_proba_index = sequence_label_proba.argmax(axis=1)\n",
    "        return self.unique_labels[max_proba_index]\n",
    "    \n",
    "    def predict_proba(self, features, metric=spatial.distance.cosine):\n",
    "        sequence_n = len(features) \n",
    "        pattern_n = len(self.patterns)\n",
    "        \n",
    "        sequence_label_proba = np.zeros((sequence_n, pattern_n), dtype=self.labels.dtype)\n",
    "        for i, sequence in enumerate(features):\n",
    "            sequence_label_proba[i, :] = self.predict_single_proba(sequence, metric)\n",
    "            \n",
    "        return sequence_label_proba\n",
    "    \n",
    "    def predict_single_proba(self, sequence, metric=spatial.distance.cosine):\n",
    "        pattern_dists = np.zeros(len(self.patterns), dtype=np.float64)\n",
    "        for i, pattern in enumerate(self.patterns):\n",
    "            distance, _ = fastdtw.fastdtw(pattern, sequence, dist=metric)\n",
    "            pattern_dists[i] = distance\n",
    "            \n",
    "        pattern_proba = np.exp(-pattern_dists)\n",
    "        \n",
    "        label_proba = np.zeros(len(self.unique_labels), dtype=np.float64)\n",
    "        all_dim = tuple(range(1, self.labels.ndim))\n",
    "        for i, label in enumerate(self.unique_labels):\n",
    "            relevant = (self.labels == label).all(axis=all_dim)\n",
    "            total_proba = pattern_proba[relevant].sum()\n",
    "            label_proba[i] = total_proba\n",
    "        \n",
    "        return label_proba / label_proba.sum()\n",
    "\n",
    "classifier = markov.HMMGMMClassifier()\n",
    "classifier.fit(e_features, e_labels, n_components=10, n_mix=2)\n",
    "return classifier.predict(t_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_male  speaker_id\n",
       "False    2             30\n",
       "         4             30\n",
       "         5             30\n",
       "         6             30\n",
       "         8             24\n",
       "         12            30\n",
       "True     1             30\n",
       "         2             30\n",
       "         4             30\n",
       "         5             30\n",
       "         6             30\n",
       "         7             30\n",
       "         8              6\n",
       "         9             30\n",
       "         13            30\n",
       "         14            24\n",
       "         15            30\n",
       "         16            30\n",
       "         17            30\n",
       "         18            30\n",
       "         19            30\n",
       "         20            30\n",
       "         21            24\n",
       "         22            30\n",
       "         23            30\n",
       "         26            30\n",
       "         28            30\n",
       "         29            30\n",
       "         32            30\n",
       "         38            24\n",
       "         40            30\n",
       "         41            30\n",
       "         43            30\n",
       "         47            30\n",
       "         48             6\n",
       "         51            30\n",
       "         52             6\n",
       "         53            30\n",
       "         54            30\n",
       "         55            30\n",
       "         60            30\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(enrollments_1.groupby(['is_male', 'speaker_id']).size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
