{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures as cf\n",
    "import functools as ft\n",
    "import itertools as it\n",
    "import json\n",
    "import math\n",
    "import operator as op\n",
    "import os\n",
    "import re\n",
    "\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interact_manual, widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy import interpolate, linalg, misc, optimize, spatial, stats\n",
    "from sklearn import metrics, mixture, cluster, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs are useful for modelling sequences. They differ from feed-forward NNs in that the output of network at time $t$ is passed as an argument at time $t+1$.\n",
    "LSTM are a special kind of RNNs.\n",
    "\n",
    "In theory, RNNs are absolutely capable of handling such “long-term dependencies.” A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs don’t seem to be able to learn them. The problem was explored in depth by Hochreiter (1991) [German] and Bengio, et al. (1994), who found some pretty fundamental reasons why it might be difficult. Thankfully, LSTMs don’t have this problem!\n",
    "\n",
    "Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.\n",
    "An LSTM has three of these gates, to protect and control the cell state.\n",
    "\n",
    "As of this writing, the most eﬀective sequence models used in practical applicationsare calledgated RNNs. These include thelong short-term memoryandnetworks based on the gated recurrent unit\n",
    "Like leaky units, gated RNNs are based on the idea of creating paths throughtime that have derivatives that neither vanish nor explode. Leaky units didthis with connection weights that were either manually chosen constants or wereparameters. Gated RNNs generalize this to connection weights that may changeat each time step.\n",
    "\n",
    "By making the weight of this self-loop gated (controlledby another hidden unit), the time scale of integration can be changed dynamically\n",
    "\n",
    "short-term memory - pamięć krótkotrwała\n",
    "\n",
    "$$X_t = [h_{t-1}, x_t]$$ konkatenacja danych wejściowych i poprzedniego wyjścia\n",
    "$$f_t = \\sigma(W_f \\cdot X_t + b_f)$$ brama zapomnienia \n",
    "$$i_t = \\sigma(W_i \\cdot X_t + b_i)$$ brama wejścia\n",
    "$$o_t = \\sigma(W_o \\cdot X_t + b_o)$$ brama wyjścia\n",
    "$$\\bar{C_t} = tanh(W_c \\cdot X_t + b_c)$$ nowy kandydat na stan w ukrytej komórce i wyjście\n",
    "$$C_t = f_t \\circ C_{t-1} + i_t \\circ \\bar{C_t}$$ nowy ukryty stan komórki\n",
    "$$h_t = o_t \\circ tanh(C_t)$$ nowe wyjście\n",
    "\n",
    "Parametrami są wagi przy bramce zapomnienia $W_f$ i $b_f$, wagi przy bramce wejścia $W_i$, $b_i$, wagi przy bramce wyjścia $W_o$, $b_0$, wagi, którymi generowany jeset nowy kandydat na stan $W_c$, $b_c$.\n",
    "\n",
    "Jeżeli wektor wejściowy $x_t$ ma rozmiar $N$, a stan ukryty $C_t$ oraz wyjście sieci $h_t$ to wektory rozmiaru $M$, to $X_t$ ma rozmiar $M + N$, wyjścia na czterech bramkach mają rozmiar $M$. Macierze parametrów na bramkach mają rozmiar $M + N \\times M$ oraz $M$.\n",
    "\n",
    "LSTM można trenować z użyciem propagacji wstecznej. Jeżeli błąd w chwili $t$ to $E_t$, to\n",
    "\n",
    "$\\frac{dE}{dW_f} = \\frac{dE}{dh_t} \\frac{dh_t}{dC_t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def identity(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM():\n",
    "    def __init__(self, forget_weights, forget_bias, input_weights, input_bias,\n",
    "                 candidate_weights, candidate_bias, output_weights, output_bias,\n",
    "                 activation=np.tanh, recurrent_activation=np.tanh):\n",
    "        self.forget_weights = forget_weights\n",
    "        self.forget_bias = forget_bias\n",
    "        self.input_weights = input_weights\n",
    "        self.input_bias = input_bias\n",
    "        self.candidate_weights = candidate_weights\n",
    "        self.candidate_bias = candidate_bias\n",
    "        self.output_weights = output_weights\n",
    "        self.output_bias = output_bias\n",
    "        self.activation = activation\n",
    "        self.recurrent_activation = recurrent_activation\n",
    "\n",
    "    def forward(self, features, last_output, last_state):\n",
    "        extended_features = np.concatenate((features, last_output))\n",
    "        forget_gate = sigmoid(self.forget_weights @ extended_features + self.forget_bias)\n",
    "        input_gate = sigmoid(self.input_weights @ extended_features + self.input_bias)\n",
    "        output_gate = sigmoid(self.output_weights @ extended_features + self.output_bias)\n",
    "        candidate = self.recurrent_activation(self.candidate_weights @ extended_features + self.candidate_bias)\n",
    "        new_state = last_state * forget_gate + candidate * input_gate\n",
    "        new_output = self.activation(new_state) * output_gate\n",
    "        return new_output, new_state\n",
    "        \n",
    "    def run(self, features, init_output, init_state):\n",
    "        output, state = init_output, init_state\n",
    "        outputs = np.zeros((features.shape[0], init_output.size), dtype=init_output.dtype)\n",
    "        for i, feature_row in enumerate(features):\n",
    "            output, state = self.forward(feature_row, output, state)\n",
    "            outputs[i] = output\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2],\n",
       "       [ 5],\n",
       "       [-4],\n",
       "       [ 0],\n",
       "       [10]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identity_lstm = LSTM(\n",
    "    np.array([[0, 0]]), np.array([-math.inf]),\n",
    "    np.array([[0, 0]]), np.array([math.inf]),\n",
    "    np.array([[1, 0]]), np.array([0]),\n",
    "    np.array([[0, 0]]), np.array([math.inf]),\n",
    "    activation=identity, recurrent_activation=identity\n",
    ")\n",
    "\n",
    "identity_lstm.run(np.array([[2], [5], [-4], [0], [10]]), np.array([0]), np.array([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_lstm = LSTM(\n",
    "    np.array([[0, 0]]), np.array([math.inf]),\n",
    "    np.array([[0, 0]]), np.array([math.inf]),\n",
    "    np.array([[0, 0]]), np.array([1]),\n",
    "    np.array([[0, 0]]), np.array([math.inf]),\n",
    "    activation=identity, recurrent_activation=identity\n",
    ")\n",
    "\n",
    "count_lstm.run(np.array([[2], [5], [-4], [0], [10]]), np.array([0]), np.array([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2],\n",
       "       [ 7],\n",
       "       [ 3],\n",
       "       [ 3],\n",
       "       [13]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_lstm = LSTM(\n",
    "    np.array([[0, 0]]), np.array([math.inf]),\n",
    "    np.array([[0, 0]]), np.array([math.inf]),\n",
    "    np.array([[1, 0]]), np.array([0]),\n",
    "    np.array([[0, 0]]), np.array([math.inf]),\n",
    "    activation=identity, recurrent_activation=identity\n",
    ")\n",
    "\n",
    "sum_lstm.run(np.array([[2], [5], [-4], [0], [10]]), np.array([0]), np.array([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.  ,  3.5 ,  1.  ,  0.75,  2.6 ])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_lstm = LSTM(\n",
    "    np.array([[0, 0, 0], [0, 0, 0]]), np.array([math.inf, math.inf]),\n",
    "    np.array([[0, 0, 0], [0, 0, 0]]), np.array([math.inf, math.inf]),\n",
    "    np.array([[1, 0, 0], [0, 0, 0]]), np.array([0, 1]),\n",
    "    np.array([[0, 0, 0], [0, 0, 0]]), np.array([math.inf, math.inf]),\n",
    "    activation=identity, recurrent_activation=identity\n",
    ")\n",
    "\n",
    "result = mean_lstm.run(np.array([[2], [5], [-4], [0], [10]]), np.array([0, 0]), np.array([0, 0]))\n",
    "result[:, 0] / result[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
