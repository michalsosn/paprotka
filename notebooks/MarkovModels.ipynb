{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Models\n",
    "\n",
    "* Forward algorithm - calculate the probability that a certain sequence of observations occurs\n",
    "* Viterbi algorithm - find the most probable sequence of hidden states given a sequence of observations\n",
    "* Baum-Welch algorithm - find the parameters of a model that maximize the likelihood of a certain sequence of observations occuring\n",
    "\n",
    "The model must have a Markov property, which means that the distribution of the next hidden state and observation can only depend on the current state or a fixed number of past states. The model is also uniform, which means that the conditional distributions don't change. It should also have a property that the probability of getting from one state to any other one is always non zero.\n",
    "\n",
    "The model has weights determining the distribution of hidden states in each hidden state, weights for distributions of observations in each given state and also an initial state (or a vector of probabilities of initial state).\n",
    "\n",
    "The hidden states seem to be usually discrete, while the observations can be discrete continuous.\n",
    "\n",
    "Number of hidden states grow exponentially with a number of features that are modelled. That is, if the model of a person speech has N states and you need to additionaly include information whether the person is male or female, you now need 2N states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools as ft\n",
    "import itertools as it\n",
    "import json\n",
    "import math\n",
    "import operator as op\n",
    "import os\n",
    "\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interact_manual, widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import misc, stats\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete Observations\n",
    "\n",
    "First I will consider a simpler case where hidden states and observations are discrete. Markov model is defined by choosing a number of hidden features $h$ and visible observations $v$ (we don't care about assigning symbols to them and will use numbers $0 \\dots h$ and $0 \\dots v$ as states. It's then necessary to define two matrices of probabilities.\n",
    "\n",
    "First is a matrix of probabilities of transitions between hidden states $H$ of size $h \\times h$ where $h$ is a number of hidden states. $H_{i, j}$ is probability of going from hidden state $i$ to hidden state $j$, for all $i$ $\\sum_{j=0}^{h-1} H_{i, j} = 1$ and for all $i, j$ $H_{i, j} > 0$.\n",
    "\n",
    "Second matrix is a matrix of probabilities of emitting observations while being in a certain state $V$ of size $h \\times v$ where $v$ is a number of possible observations (visible states). $V_{i, k}$ is a probability of emitting observation $k$ while in hidden state $i$. For all $i$ $\\sum_{k=0}^{v-1} V_{i, k} = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  1.  0.]\n",
      "[ 0.25  0.25  0.25  0.25]\n",
      "[ 0.1  0.1  0.7  0.1]\n"
     ]
    }
   ],
   "source": [
    "hidden_n = 4\n",
    "visible_n = 3\n",
    "\n",
    "# square matrix h x h, each row sums to 1\n",
    "hidden_weights = np.array([[0.1, 0.4, 0.4, 0.1], \n",
    "                           [0.2, 0.2, 0.2, 0.4], \n",
    "                           [0.4, 0.2, 0.3, 0.1], \n",
    "                           [0.3, 0.3, 0.1, 0.3]]) \n",
    "# matrix h x v, each row sums to 1\n",
    "visible_weights = np.array([[0.3, 0.4, 0.3], \n",
    "                            [0.1, 0.9, 0.0], \n",
    "                            [0.5, 0.4, 0.1],\n",
    "                            [0.25, 0.1, 0.65]]) \n",
    "\n",
    "def hidden_selected(hidden_n, index):\n",
    "    hidden_states = np.zeros(hidden_n, dtype=np.float64)\n",
    "    hidden_states[index] = 1.0\n",
    "    return hidden_states\n",
    "\n",
    "print(hidden_selected(hidden_n, 2))\n",
    "\n",
    "def hidden_uniform(hidden_n):\n",
    "    return np.ones(hidden_n, dtype=np.float64) / hidden_n\n",
    "\n",
    "print(hidden_uniform(hidden_n))\n",
    "\n",
    "initial_hidden = np.array([0.1, 0.1, 0.7, 0.1])\n",
    "print(initial_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.4  0.2  0.3  0.1]\n",
      "[ 0.25  0.25  0.25  0.25]\n",
      "[ 0.34  0.22  0.28  0.16]\n"
     ]
    }
   ],
   "source": [
    "def generate_visible(visible_weights, hidden_state):\n",
    "    return hidden_weights @ hidden_state\n",
    "\n",
    "print(generate_visible(visible_weights, hidden_selected(hidden_n, 2)))\n",
    "print(generate_visible(visible_weights, hidden_uniform(hidden_n)))\n",
    "print(generate_visible(visible_weights, initial_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (1, 0), (1, 1), (2, 3), (1, 0), (1, 1), (2, 3), (1, 0), (1, 1), (2, 3), (1, 0), (1, 1), (2, 3), (1, 0), (1, 1), (2, 3), (1, 0), (1, 1), (2, 3), (1, 0), (1, 1), (2, 3), (1, 0), (1, 1), (2, 3)]\n",
      "[(2, 3), (2, 3), (1, 1), (0, 2), (1, 2), (0, 2), (0, 0), (0, 2), (1, 1), (0, 3), (2, 0), (0, 2), (0, 2), (1, 1), (2, 2), (2, 2), (1, 1), (0, 0), (1, 1), (0, 2), (2, 2), (1, 1), (2, 3), (1, 1), (2, 3)]\n"
     ]
    }
   ],
   "source": [
    "def walk_max_likely(hidden_weights, visible_weights, initial_hidden):\n",
    "    max_hidden_per_hidden = np.argmax(hidden_weights, axis=1)\n",
    "    max_visible_per_hidden = np.argmax(visible_weights, axis=1)\n",
    "    \n",
    "    hidden_max = np.argmax(initial_hidden)\n",
    "    while True:\n",
    "        visible_max = max_visible_per_hidden[hidden_max]\n",
    "        yield visible_max, hidden_max\n",
    "        hidden_max = max_hidden_per_hidden[hidden_max]\n",
    "        \n",
    "walk = walk_max_likely(hidden_weights, visible_weights, initial_hidden)\n",
    "print(list(it.islice(walk, 25)))\n",
    "\n",
    "\n",
    "def walk_random(hidden_weights, visible_weights, initial_hidden):\n",
    "    hidden_range = np.arange(hidden_weights.shape[1])\n",
    "    visible_range = np.arange(visible_weights.shape[1])\n",
    "    \n",
    "    hidden_chosen = np.random.choice(hidden_range, p=initial_hidden)\n",
    "    while True:\n",
    "        visible_chosen = np.random.choice(visible_range, p=visible_weights[hidden_chosen, :])\n",
    "        yield visible_chosen, hidden_chosen\n",
    "        hidden_chosen = np.random.choice(hidden_range, p=hidden_weights[hidden_chosen, :])\n",
    "        \n",
    "walk = walk_random(hidden_weights, visible_weights, initial_hidden)\n",
    "print(list(it.islice(walk, 25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Algorithm\n",
    "\n",
    "Wiki articles on this are actually quite long and good: [wiki/Baum-Welch](https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm), [wiki/Forward algorithm](https://en.wikipedia.org/wiki/Forward_algorithm).\n",
    "\n",
    "Calculate probability of a given state given that a certain sequence of observations occured in the past.\n",
    "\n",
    "To be exact it calculates $P(x_t, y_t \\dots y_1)$. It does so with recursion \n",
    "\n",
    "$$P(x_t, y_t \\dots y_1) \n",
    "= \\sum_{x_{t-1}} P(x_t, x_{t-1}, y_t \\dots y_1) \n",
    "= \\sum_{x_{t-1}} P(y_t | x_t, x_{t-1}, y_{t-1} \\dots y_1) P(x_t | x_{t-1}, y_{t-1} \\dots y_1) P(x_{t-1}, y_{t-1} \\dots y_1)$$\n",
    "\n",
    "Now, thanks to Markov property we can simplify\n",
    "\n",
    "$$P(y_t | x_t, x_{t-1}, y_{t-1} \\dots y_1) = P(y_t | x_t) = V_{x_t, y_t}$$  \n",
    "\n",
    "$$P(x_t | x_{t-1}, y_{t-1} \\dots y_1) = P(x_t | x_{t-1}) = H_{x_{t-1}, x_t}$$\n",
    "\n",
    "And the final equation is\n",
    "\n",
    "$$P(x_t, y_t \\dots y_1) = V_{x_t, y_t} \\sum_{x_{t-1}} H_{x_{t-1}, x_t} P(x_{t-1}, y_{t-1} \\dots y_1) $$\n",
    "\n",
    "Where $P(x_{t-1}, y_{t-1} \\dots y_1)$ can be recursively calculated in the same way until we get to $P(x_0, \\emptyset) = \\pi_1$ and $\\pi$ is a given vector of size $h$ defining the initial state.\n",
    "\n",
    "<!-- or this $P(x_1, y_1) = P(y_1 | x_1) P(x_1) = V_{x_1,y_1} \\pi_{x_1}$ ? -->\n",
    "\n",
    "Using Markov property reduces complexity from $\\theta(nh^n)$ to $\\theta(nh^2)$, where $n$ is a number of observations.\n",
    "\n",
    "I keep all intermediate states in my implementation to visualize the progress better. I don't really plan to make a production grade system here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.56039699457e-07\n",
      "[[  1.00000000e-01   1.00000000e-01   7.00000000e-01   1.00000000e-01]\n",
      " [  1.36000000e-01   2.07000000e-01   1.12000000e-01   1.50000000e-02]\n",
      " [  4.17200000e-02   1.10430000e-01   5.23600000e-02   1.12100000e-02]\n",
      " [  2.02260000e-02   4.73481000e-02   2.22412000e-02   5.69430000e-03]\n",
      " [  8.83879600e-03   2.13448950e-02   9.92072400e-03   2.48942500e-03]\n",
      " [  3.94719028e-03   9.48192273e-03   4.41186284e-03   1.11607375e-03]\n",
      " [  1.75626833e-03   4.22320982e-03   1.96417075e-03   4.96349653e-04]\n",
      " [  7.81936798e-04   1.87999951e-03   8.74414195e-04   2.21023273e-04]\n",
      " [  2.61079872e-04   0.00000000e+00   9.73201207e-05   6.39562225e-04]\n",
      " [  1.02761881e-04   2.84188177e-04   7.90336831e-05   2.27708667e-05]\n",
      " [  4.22234227e-05   1.08522346e-04   4.95718318e-05   1.38686087e-05]\n",
      " [  1.99664507e-05   4.74019085e-05   2.19408995e-05   5.67490465e-06]\n",
      " [  6.58675739e-06   0.00000000e+00   2.46167223e-06   1.61550804e-05]\n",
      " [  1.94696062e-06   0.00000000e+00   4.98871266e-07   3.73838860e-06]\n",
      " [  4.54728345e-07   2.00007508e-07   6.51142245e-07   3.41524942e-07]\n",
      " [  1.79355487e-07   4.09120894e-07   1.80555203e-07   2.93047545e-08]\n",
      " [  7.23092940e-08   1.78621956e-07   8.42653639e-08   2.08430853e-08]]\n",
      "0.046731\n",
      "[[ 0.        1.        0.        0.      ]\n",
      " [ 0.06      0.        0.02      0.26    ]\n",
      " [ 0.0368    0.0954    0.0224    0.0086  ]\n",
      " [ 0.01029   0.004086  0.02069   0.011665]]\n",
      "0.01898925\n",
      "[[ 1.          0.          0.          0.        ]\n",
      " [ 0.03        0.          0.04        0.065     ]\n",
      " [ 0.0154      0.03555     0.0122      0.00265   ]\n",
      " [ 0.0042975   0.0016505   0.0085975   0.00444375]]\n"
     ]
    }
   ],
   "source": [
    "def forward_probability(hidden_weights, visible_weights, initial_hidden, observations):\n",
    "    joint_probs = np.zeros((observations.size + 1, initial_hidden.size), dtype=np.float64)\n",
    "    joint_probs[0, :] = initial_hidden\n",
    "    \n",
    "    for i, observation in enumerate(observations):\n",
    "        joint_probs[i + 1] = (joint_probs[i] @ hidden_weights) * visible_weights[:, observation]\n",
    "            \n",
    "    return joint_probs[-1].sum(), joint_probs\n",
    "    \n",
    "walk = walk_random(hidden_weights, visible_weights, initial_hidden)\n",
    "observations = np.array([v for v, h in it.islice(walk, 16)])\n",
    "print(*forward_probability(hidden_weights, visible_weights, initial_hidden, observations), sep='\\n')\n",
    "\n",
    "observations = np.array([2, 1, 0])\n",
    "print(*forward_probability(hidden_weights, visible_weights, hidden_selected(4, 1), observations), sep='\\n')\n",
    "\n",
    "observations = np.array([2, 1, 0])\n",
    "print(*forward_probability(hidden_weights, visible_weights, hidden_selected(4, 0), observations), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Algorithm\n",
    "\n",
    "It's similar to forward algorithm. It calculates $P(y_{t+1} \\dots y_T | x_t)$ that is probability of future sequence of observations $y_{t+1} \\dots y_T$ given initial state $x_t$ at time $t$. \n",
    "\n",
    "It starts with the final state $x_T$.\n",
    "\n",
    "$$P(y_{t+1} \\dots y_{T} | x_t)\n",
    "= \\sum_{x_{t+1}} P(y_{t+1} \\dots y_{T}, x_{t+1} | x_t)\n",
    "= \\sum_{x_{t+1}} P(y_{t+1} | y_{t+2} \\dots y_{T}, x_{t+1}, x_t) P(y_{t+2} \\dots y_{T} | x_{t+1}, x_t) P(x_{t+1} | x_t)$$\n",
    "\n",
    "Now we can hopefully simplify:\n",
    "\n",
    "$$P(y_{t+1} | y_{t+2} \\dots y_{T}, x_{t+1}, x_t) = P(y_{t+1} | x_{t+1}) = V_{x_{t+1},y_{t+1}}$$\n",
    "\n",
    "$$P(y_{t+2} \\dots y_{T} | x_{t+1}, x_t) = P(y_{t+2} \\dots y_{T} | x_{t+1})$$\n",
    "\n",
    "$$P(x_{t+1} | x_t) = H_{x_t,x_{t+1}}$$\n",
    "\n",
    "And the end result is:\n",
    "\n",
    "$$P(y_{t+1} \\dots y_{T} | x_t) =  \\sum_{x_{t+1}} V_{x_{t+1},y_{t+1}} H_{x_t,x_{t+1}} P(y_{t+2} \\dots y_{T} | x_{t+1})$$\n",
    "\n",
    "The probabilities are computed recursively. Base case is $P(\\emptyset | x_T) = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.32360852066e-07\n",
      "[[  1.44303052e-07   1.26907056e-07   1.51407262e-07   1.09743482e-07]\n",
      " [  4.63201642e-07   4.40443395e-07   5.15501399e-07   3.87559496e-07]\n",
      " [  1.75650861e-06   1.24308285e-06   1.61848723e-06   1.48342499e-06]\n",
      " [  4.16099094e-06   2.75610462e-06   3.53103658e-06   3.29054555e-06]\n",
      " [  7.77487839e-06   6.83753007e-06   8.30460889e-06   5.97475557e-06]\n",
      " [  2.64666440e-05   2.32697110e-05   2.77374648e-05   2.01039074e-05]\n",
      " [  8.46054189e-05   8.06485493e-05   9.46139183e-05   7.11902301e-05]\n",
      " [  3.25613410e-04   2.27982736e-04   2.94741055e-04   2.70779844e-04]\n",
      " [  7.31556909e-04   5.12128753e-04   6.61900222e-04   6.08074760e-04]\n",
      " [  1.64103516e-03   1.15025128e-03   1.48839188e-03   1.36823397e-03]\n",
      " [  3.71326108e-03   2.58607620e-03   3.31852224e-03   3.05537255e-03]\n",
      " [  8.00821555e-03   5.83637700e-03   7.62942115e-03   7.11293505e-03]\n",
      " [  2.11782050e-02   1.30002200e-02   1.45034850e-02   1.60450550e-02]\n",
      " [  1.82995000e-02   4.28100000e-02   2.91215000e-02   3.75185000e-02]\n",
      " [  1.37800000e-01   1.34800000e-01   1.58000000e-01   1.20700000e-01]\n",
      " [  5.70000000e-01   3.80000000e-01   4.70000000e-01   4.60000000e-01]\n",
      " [  1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00]]\n",
      "0.013276849375\n",
      "[[ 0.00173233  0.00512761  0.00229158  0.00412533]\n",
      " [ 0.00729638  0.0214205   0.00974238  0.01728838]\n",
      " [ 0.031825    0.0891      0.041825    0.071825  ]\n",
      " [ 0.135       0.34        0.215       0.295     ]\n",
      " [ 1.          1.          1.          1.        ]]\n",
      "0.4721\n",
      "[[ 0.4096  0.      0.0625]\n",
      " [ 0.512   0.      0.125 ]\n",
      " [ 0.64    0.      0.25  ]\n",
      " [ 0.8     0.      0.5   ]\n",
      " [ 1.      1.      1.    ]]\n"
     ]
    }
   ],
   "source": [
    "def backward_probability(hidden_weights, visible_weights, observations):\n",
    "    conditional_probs = np.zeros((observations.size + 1, hidden_weights.shape[0]), dtype=np.float64)\n",
    "    conditional_probs[-1, :] = 1\n",
    "    \n",
    "    for i, observation in reversed(list(enumerate(observations))):\n",
    "        conditional_probs[i] = (conditional_probs[i + 1] * visible_weights[:, observation]) @ hidden_weights.T\n",
    "            \n",
    "    return conditional_probs[0].sum(), conditional_probs\n",
    "    \n",
    "walk = walk_random(hidden_weights, visible_weights, initial_hidden)\n",
    "observations = np.array([v for v, h in it.islice(walk, 16)])\n",
    "print(*backward_probability(hidden_weights, visible_weights, observations), sep='\\n')\n",
    "\n",
    "observations = np.array([2, 2, 2, 2])\n",
    "print(*backward_probability(hidden_weights, visible_weights, observations), sep='\\n')\n",
    "\n",
    "observations = np.array([1, 1, 1, 1])\n",
    "print(*backward_probability(np.eye(3), np.array([[0.2, 0.8], [1.0, 0.0], [0.5, 0.5]]), observations), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward-Backward Algorithm\n",
    "\n",
    "For any $k = 1 \\dots t$\n",
    "\n",
    "* Forward algorithm calculates $P(x_t, y_t \\dots y_1)$. It calculates probability of a hidden state and a sequence of past observations at time $t$.\n",
    "* Backward algorithm calculates $P(y_T \\dots y_{t+1} | x_t)$. It calculates probability of a sequence of future observations $y_{t+1} \\dots y_T$ given a current state $x_t$ at time $t$.\n",
    "* Forward-backward algorithm calculates $P(x_t | y_T \\dots y_1)$. So it gives probability of any hidden state $x_t$ at any time taking into accout full sequence of observations. It can then at any time select the most likely state. Note that by selecting the most likely state we will not necessarily get the most likely sequence. Viterbi algorithms is needed for that.\n",
    "\n",
    "Forward-backward algorithm combines the results of the previous two algorithms.\n",
    "\n",
    "$$P(x_t, y_t \\dots y_1) P(y_T \\dots y_{t+1} | x_t) \n",
    "= P(y_t \\dots y_1 | x_t) P(y_T \\dots y_{t+1} | x_t) P(x_t) \n",
    "= P(y_T \\dots y_1 | x_t) P(x_t) \n",
    "= P(y_T \\dots y_1, x_t) \n",
    "= P(x_t | y_T \\dots y_1) P(y_T \\dots y_1)$$\n",
    "\n",
    "$$P(x_t | y_T \\dots y_1) = \\frac{P(x_t, y_t \\dots y_1) P(y_T \\dots y_{t+1} | x_t)}{P(y_T \\dots y_1)}$$\n",
    "\n",
    "$P(y_T \\dots y_1)$ can be calculated as a marginal distribution of $\\sum_{x_T} P(x_T, y_T \\dots y_1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10186777  0.0896863   0.73204128  0.07640464]\n",
      " [ 0.3085618   0.07162325  0.51207468  0.10774027]\n",
      " [ 0.35582238  0.07491621  0.44794118  0.12132024]\n",
      " [ 0.12190915  0.62833388  0.21392863  0.03582834]\n",
      " [ 0.31029617  0.          0.10113975  0.58856408]\n",
      " [ 0.21377889  0.56187313  0.18477054  0.03957744]\n",
      " [ 0.25325211  0.06750127  0.44891593  0.23033068]\n",
      " [ 0.29361925  0.42869215  0.24106035  0.03662825]\n",
      " [ 0.2410574   0.43381504  0.26596914  0.05915842]\n",
      " [ 0.27110046  0.42104765  0.2481342   0.0597177 ]\n",
      " [ 0.2062698   0.48005598  0.2635385   0.05013572]\n",
      " [ 0.28070503  0.06618658  0.44342586  0.20968253]\n",
      " [ 0.25175911  0.47043376  0.24918719  0.02861994]\n",
      " [ 0.23282503  0.08033678  0.5022377   0.18460049]\n",
      " [ 0.32057354  0.09561011  0.45672954  0.12708682]\n",
      " [ 0.17694599  0.14702262  0.49377463  0.18225676]\n",
      " [ 0.3964654   0.          0.13615381  0.46738079]]\n",
      "[[  5.26025456e-09   4.63122702e-09   3.78011937e-08   3.94538757e-09]\n",
      " [  1.59335335e-08   3.69848583e-09   2.64425446e-08   5.56349884e-09]\n",
      " [  1.83739782e-08   3.86852781e-09   2.31308146e-08   6.26474217e-09]\n",
      " [  6.29515243e-09   3.24459444e-08   1.10468599e-08   1.85010610e-09]\n",
      " [  1.60230931e-08   0.00000000e+00   5.22266085e-09   3.03923089e-08]\n",
      " [  1.10391277e-08   2.90140401e-08   9.54119272e-09   2.04370234e-09]\n",
      " [  1.30774485e-08   3.48563484e-09   2.31811492e-08   1.18938303e-08]\n",
      " [  1.51619293e-08   2.21368319e-08   1.24478896e-08   1.89141194e-09]\n",
      " [  1.24477373e-08   2.24013685e-08   1.37341309e-08   3.05482612e-09]\n",
      " [  1.39991025e-08   2.17420849e-08   1.28131692e-08   3.08370625e-09]\n",
      " [  1.06513727e-08   2.47891610e-08   1.36086175e-08   2.58891161e-09]\n",
      " [  1.44950640e-08   3.41774689e-09   2.28976524e-08   1.08275995e-08]\n",
      " [  1.30003529e-08   2.42922880e-08   1.28675437e-08   1.47787823e-09]\n",
      " [  1.20226337e-08   4.14843573e-09   2.59345819e-08   9.53241156e-09]\n",
      " [  1.65537964e-08   4.93712064e-09   2.35846285e-08   6.56251722e-09]\n",
      " [  9.13714816e-09   7.59196337e-09   2.54975652e-08   9.41138605e-09]\n",
      " [  2.04727051e-08   0.00000000e+00   7.03071907e-09   2.41346386e-08]]\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "\n",
      "[[ 0.          1.          0.          0.        ]\n",
      " [ 0.21242858  0.          0.05899724  0.72857418]\n",
      " [ 0.23230832  0.57161199  0.15099185  0.04508784]\n",
      " [ 0.22019644  0.08743661  0.44274678  0.24962017]]\n",
      "[[ 0.        0.046731  0.        0.      ]\n",
      " [ 0.009927  0.        0.002757  0.034047]\n",
      " [ 0.010856  0.026712  0.007056  0.002107]\n",
      " [ 0.01029   0.004086  0.02069   0.011665]]\n",
      "\n",
      "[[ 1.          0.          0.          0.        ]\n",
      " [ 0.26138473  0.          0.29037482  0.44824045]\n",
      " [ 0.23924062  0.52419132  0.20237766  0.0341904 ]\n",
      " [ 0.22631226  0.0869176   0.45275616  0.23401398]]\n",
      "[[ 0.01898925  0.          0.          0.        ]\n",
      " [ 0.0049635   0.          0.005514    0.00851175]\n",
      " [ 0.004543    0.009954    0.003843    0.00064925]\n",
      " [ 0.0042975   0.0016505   0.0085975   0.00444375]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def forward_backward_probability(hidden_weights, visible_weights, initial_hidden, observations):\n",
    "    _, forward_result = forward_probability(hidden_weights, visible_weights, initial_hidden, observations)\n",
    "    _, backward_result = backward_probability(hidden_weights, visible_weights, observations)\n",
    "    scaling_coefficient = forward_result[-1, :].sum()\n",
    "    combined_result = forward_result * backward_result\n",
    "    return combined_result / scaling_coefficient, combined_result\n",
    "\n",
    "walk = walk_random(hidden_weights, visible_weights, initial_hidden)\n",
    "observations = np.array([v for v, h in it.islice(walk, 16)])\n",
    "print(*forward_backward_probability(hidden_weights, visible_weights, initial_hidden, observations), sep='\\n')\n",
    "print(forward_backward_probability(hidden_weights, visible_weights, initial_hidden, observations)[0].sum(axis=1))\n",
    "print()\n",
    "\n",
    "observations = np.array([2, 1, 0])\n",
    "print(*forward_backward_probability(hidden_weights, visible_weights, hidden_selected(4, 1), observations), sep='\\n')\n",
    "print()\n",
    "\n",
    "observations = np.array([2, 1, 0])\n",
    "print(*forward_backward_probability(hidden_weights, visible_weights, hidden_selected(4, 0), observations), sep='\\n')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baum-Welch Algorithm\n",
    "\n",
    "Estimate the most likely weights $H$, $V$ and $\\pi$ (initial state) of a model given sequences of observations.\n",
    "\n",
    "It fits the weights using Expectation Maximization. The forward-backward algorithm is used to find the likelihoods of hidden states given a sequence of observations.\n",
    "\n",
    "We proved previously that forward-backward algorithm can calculate $P(x_t | y_T \\dots y_1)$ as:\n",
    "\n",
    "$$P(x_t | y_T \\dots y_1) = \\frac{P(x_t, y_t \\dots y_1) P(y_T \\dots y_{t+1} | x_t)}{P(y_T \\dots y_1)}$$\n",
    "\n",
    "where $P(x_t, y_t \\dots y_1)$ $P(y_T \\dots y_{t+1} | x_t)$ are given to us by forward and backward algorithms and $P(y_T \\dots y_1)$ can be calculated as a marginal distribution of $\\sum_{x_T} P(x_T, y_T \\dots y_1)$\n",
    "\n",
    "To perform Baum-Welch learning we additionally need $P(x_t, x_{t+1} | y_T \\dots y_1)$, which is the probability of transitioning from state $x_t$ at time $t$ to state $x_{t+1}$ at time $t+1$ given the sequence of observations. (and implicitly given model parameters)\n",
    "\n",
    "$$P(x_t, x_{t+1} | y_T \\dots y_1)\n",
    "= \\frac{P(x_t, x_{t+1}, y_T \\dots y_1)}{P(y_T \\dots y_1)}$$\n",
    "\n",
    "$$P(x_t, x_{t+1}, y_T \\dots y_1) \\\\\n",
    "= P(y_T \\dots y_1 | x_t, x_{t+1}) P(x_{t+1} | x_t) P(x_t) \\\\\n",
    "= P(y_T \\dots y_{t+2} | x_t, x_{t+1}) P(y_{t+1} \\dots y_1 | x_t, x_{t+1}) P(x_{t+1} | x_t) P(x_t)$$\n",
    "\n",
    "$$P(y_{t+1} \\dots y_1 | x_t, x_{t+1}) P(x_t) \n",
    "= P(y_{t+1} \\dots y_1, x_t | x_{t+1})\n",
    "= P(y_{t+1} | x_{t+1}) P(y_t \\dots y_1, x_t | x_{t+1})$$\n",
    "\n",
    "$$P(x_t, x_{t+1}, y_T \\dots y_1) \\\\\n",
    "= P(y_T \\dots y_{t+2} | x_t, x_{t+1}) P(y_{t+1} | x_{t+1}) P(y_t \\dots y_1, x_t | x_{t+1}) P(x_{t+1} | x_t) \\\\\n",
    "= P(y_T \\dots y_{t+2} | x_{t+1}) P(y_{t+1} | x_{t+1}) P(y_t \\dots y_1, x_t) P(x_{t+1} | x_t)$$\n",
    "\n",
    "$$P(x_t, x_{t+1} | y_T \\dots y_1)\n",
    "= \\frac{P(y_T \\dots y_{t+2} | x_{t+1}) P(y_t \\dots y_1, x_t) V_{x_{t+1}, y_{t+1}} H_{x_t, x_{t+1}}}{P(y_T \\dots y_1)}$$\n",
    "\n",
    "Then the updates are performed like this:\n",
    "\n",
    "$$\\pi_i = P(X_0 = x_i | y_T \\dots y_1)$$\n",
    "\n",
    "$$H_{x_t, x_{t+1}} = \\frac{\\sum_{t=0}^{T-1} P(x_t, x_{t+1} | y_T \\dots y_1)}{\\sum_{t=0}^{T-1} P(x_t | y_T \\dots y_1)}$$\n",
    "\n",
    "$$V_{x_t, y_t} = \\frac{\\sum_{t=1}^T \\mathbb{1}_{Y_t = y_t} P(x_t | y_T \\dots y_1)}{\\sum_{t=1}^T P(x_t | y_T \\dots y_1)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10750337  0.07842275  0.71835286  0.09572102]\n",
      " [ 0.38912982  0.3619715   0.21674052  0.03215817]\n",
      " [ 0.09448092  0.63579189  0.21138663  0.05834056]\n",
      " [ 0.32311766  0.          0.08035128  0.59653107]\n",
      " [ 0.10437996  0.71397077  0.13142459  0.05022467]\n",
      " [ 0.25414358  0.          0.0952977   0.65055873]\n",
      " [ 0.3045358   0.49890264  0.14357843  0.05298312]\n",
      " [ 0.0905138   0.69642764  0.14173629  0.07132227]\n",
      " [ 0.13649053  0.          0.06680439  0.79670508]\n",
      " [ 0.17787978  0.          0.05945295  0.76266727]\n",
      " [ 0.17470291  0.          0.08371677  0.74158032]\n",
      " [ 0.34989683  0.          0.08949147  0.5606117 ]\n",
      " [ 0.24644492  0.51291578  0.19415459  0.04648471]\n",
      " [ 0.25635556  0.42686463  0.25065029  0.06612952]\n",
      " [ 0.22190889  0.47036548  0.25896899  0.04875664]\n",
      " [ 0.24288935  0.08238282  0.48438773  0.1903401 ]\n",
      " [ 0.29680359  0.09288036  0.46359892  0.14671713]]\n",
      "[[ 0.3381161   0.80305689  1.29423942  1.22826149]\n",
      " [ 1.5242063   1.00844826  0.71841482  1.24140412]\n",
      " [ 1.11422263  0.68366092  0.87699798  0.29685999]\n",
      " [ 0.49782865  1.98284981  0.33684325  2.05458936]]\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "[ 3.66367389  4.4924735   2.97174153  4.87211107]\n",
      "\n",
      "new initial\n",
      "[ 0.10750337  0.07842275  0.71835286  0.09572102]\n",
      "1.0\n",
      "new hidden\n",
      "[[ 0.09228881  0.21919442  0.35326272  0.33525405]\n",
      " [ 0.33927998  0.22447506  0.15991521  0.27632976]\n",
      " [ 0.37493928  0.23005397  0.29511247  0.09989428]\n",
      " [ 0.10217925  0.4069796   0.06913702  0.42170413]]\n",
      "[ 0.90868731  1.08070305  0.87742742  1.13318222]\n",
      "[ 1.  1.  1.  1.]\n",
      "new visible\n",
      "[[ 0.14730922  0.46613037  0.38656041]\n",
      " [ 0.03901262  0.96098738  0.        ]\n",
      " [ 0.31900037  0.52112215  0.15987748]\n",
      " [ 0.06918094  0.08751846  0.8433006 ]]\n",
      "[ 0.57450315  2.03575836  1.38973849]\n",
      "[ 1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "def forward_backward_baum_welch(hidden_weights, visible_weights, initial_hidden, observations):\n",
    "    _, forward_result = forward_probability(hidden_weights, visible_weights, initial_hidden, observations)\n",
    "    _, backward_result = backward_probability(hidden_weights, visible_weights, observations)\n",
    "    scaling_coefficient = forward_result[-1, :].sum()\n",
    "    \n",
    "    per_state_result = forward_result * backward_result # observation+1 x state\n",
    "    per_state_result /= scaling_coefficient\n",
    "    \n",
    "    # state x state (we can sum over time/observations)\n",
    "#     per_pair_result = (forward_result[:-1].T @ (backward_result[1:] * visible_weights[:, observations].T)) * hidden_weights\n",
    "    per_pair_result = ((backward_result[1:] * visible_weights[:, observations].T).T @ forward_result[:-1]) * hidden_weights.T\n",
    "    per_pair_result /= scaling_coefficient\n",
    "    \n",
    "    return per_state_result, per_pair_result\n",
    "\n",
    "def estimate_params_baum_welch(per_state_result, per_pair_result, observations, visible_n):\n",
    "    hidden_n = per_pair_result.shape[0]\n",
    "    \n",
    "    new_initial_hidden = per_state_result[0]\n",
    "    \n",
    "    per_state_rest = per_state_result[1:]\n",
    "    per_state_rest_total = per_state_rest.sum(axis=0)\n",
    "    \n",
    "    new_hidden_weights = per_pair_result / per_state_rest_total[:, np.newaxis]\n",
    "    \n",
    "    new_visible_weights = np.zeros((hidden_n, visible_n), dtype=np.float64)\n",
    "    for i in range(new_visible_weights.shape[1]):\n",
    "#         print('DEBUG', new_visible_weights, per_state_rest.shape, observations.shape, sep='\\n', end='\\nEND\\n\\n')\n",
    "        new_visible_weights[:, i] = per_state_rest[observations == i].sum(axis=0) / per_state_rest_total\n",
    "        \n",
    "    return new_hidden_weights, new_visible_weights, new_initial_hidden\n",
    "\n",
    "walk = walk_random(hidden_weights, visible_weights, initial_hidden)\n",
    "observations = np.array([v for v, h in it.islice(walk, 16)])\n",
    "result = forward_backward_baum_welch(hidden_weights, visible_weights, initial_hidden, observations)\n",
    "print(*result, result[0].sum(axis=1), result[1].sum(axis=1), sep='\\n')\n",
    "print()\n",
    "\n",
    "new_hidden_weights, new_visible_weights, new_initial_hidden = \\\n",
    "    estimate_params_baum_welch(*result, observations, visible_weights.shape[1])\n",
    "\n",
    "print('new initial', new_initial_hidden, new_initial_hidden.sum(), sep='\\n')\n",
    "print('new hidden', new_hidden_weights, new_hidden_weights.sum(axis=0), new_hidden_weights.sum(axis=1), sep='\\n')\n",
    "print('new visible', new_visible_weights, new_visible_weights.sum(axis=0), new_visible_weights.sum(axis=1), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator\n",
      "[[ 0.1  0.4  0.4  0.1]\n",
      " [ 0.2  0.2  0.2  0.4]\n",
      " [ 0.4  0.2  0.3  0.1]\n",
      " [ 0.3  0.3  0.1  0.3]]\n",
      "[[ 0.3   0.4   0.3 ]\n",
      " [ 0.1   0.9   0.  ]\n",
      " [ 0.5   0.4   0.1 ]\n",
      " [ 0.25  0.1   0.65]]\n",
      "[ 0.1  0.1  0.7  0.1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def estimate_weights_baum_welch(hidden_n, visible_n, observations_batch, iterations,\n",
    "                                hidden_weights=None, visible_weights=None, initial_hidden=None):\n",
    "    if hidden_weights is None:\n",
    "        hidden_weights = np.random.uniform(size=(hidden_n, hidden_n))\n",
    "        hidden_weights /= hidden_weights.sum(axis=1)[:, np.newaxis]\n",
    "    if visible_weights is None:\n",
    "        visible_weights = np.random.uniform(size=(hidden_n, visible_n))\n",
    "        visible_weights /= visible_weights.sum(axis=1)[:, np.newaxis]\n",
    "    if initial_hidden is None:\n",
    "        initial_hidden = np.random.uniform(size=hidden_n)\n",
    "        initial_hidden /= initial_hidden.sum()\n",
    "        \n",
    "    batch_n = len(observations_batch)\n",
    "    batch_hidden_weights = np.zeros((batch_n, hidden_n, hidden_n), dtype=np.float64)\n",
    "    batch_visible_weights = np.zeros((batch_n, hidden_n, visible_n), dtype=np.float64)\n",
    "    batch_initial_hidden = np.zeros((batch_n, hidden_n), dtype=np.float64)\n",
    "    for i in range(iterations):\n",
    "        for j, observations in enumerate(observations_batch):\n",
    "            per_state_result, per_pair_result = \\\n",
    "                forward_backward_baum_welch(hidden_weights, visible_weights, initial_hidden, observations)\n",
    "            batch_hidden_weights[j], batch_visible_weights[j], batch_initial_hidden[j] = \\\n",
    "                estimate_params_baum_welch(per_state_result, per_pair_result, observations, visible_n)\n",
    "        hidden_weights = batch_hidden_weights.mean(axis=0)\n",
    "        visible_weights = batch_visible_weights.mean(axis=0)\n",
    "        initial_hidden = batch_initial_hidden.mean(axis=0)\n",
    "            \n",
    "    return hidden_weights, visible_weights, initial_hidden\n",
    "\n",
    "observations_batch = []\n",
    "for i in range(1000):\n",
    "    walk = walk_random(hidden_weights, visible_weights, initial_hidden)\n",
    "    observations = np.array([v for v, h in it.islice(walk, int(np.random.uniform(30, 40)))])\n",
    "    observations_batch.append(observations)\n",
    "# print('Observations', *observations_batch, sep='\\n', end='\\n\\n')\n",
    "\n",
    "print('Generator', hidden_weights, visible_weights, initial_hidden, sep='\\n', end='\\n\\n')\n",
    "\n",
    "result = estimate_weights_baum_welch(4, 3, observations_batch, 1000)\n",
    "print('Result', *result, sep='\\n', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi Algorithm\n",
    "\n",
    "Calculate the most likely sequence of hidden states given a sequence of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_likely_hidden_viterbi(hidden_weights, visible_weights, initial_hidden, observations):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM Observations\n",
    "\n",
    "It seems that in practice it's extremely common to use continuous visible states, with continuous distributions. Gaussiam Mixture Models seem to be a good model of speech.\n",
    "\n",
    "https://kastnerkyle.github.io/posts/single-speaker-word-recognition-with-hidden-markov-models/ - overview on how to use HMM for word recognition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
