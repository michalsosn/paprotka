{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Models\n",
    "\n",
    "Some resources I found:\n",
    "* [A Tutorial on HMM and Selected Applications in Speech Recognition - Rabiner](http://www.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf)\n",
    "* [Hidden Markov Models - Brown](http://cs.brown.edu/research/ai/dynamics/tutorial/Documents/HiddenMarkovModels.html)\n",
    "* https://www.autonlab.org/_media/tutorials/hmm14.pdf\n",
    "* http://www.kanungo.com/software/hmmtut.pdf\n",
    "* Wiki articles about Markov Models are actually very detailed\n",
    "\n",
    "Algorithms connected to HMM:\n",
    "\n",
    "* Forward algorithm - calculate the probability that a certain sequence of observations occurs\n",
    "* Viterbi algorithm - find the most probable sequence of hidden states given a sequence of observations\n",
    "* Baum-Welch algorithm - find the parameters of a model that maximize the likelihood of a certain sequence of observations occuring\n",
    "\n",
    "The model must have a Markov property, which means that the distribution of the next hidden state and observation can only depend on the current state or a fixed number of past states. The model is also uniform, which means that the conditional distributions don't change. It should also have a property that the probability of getting from one state to any other one is always non zero.\n",
    "\n",
    "The model has weights determining the distribution of hidden states in each hidden state, weights for distributions of observations in each given state and also an initial state (or a vector of probabilities of initial state).\n",
    "\n",
    "The hidden states seem to be usually discrete, while the observations can be discrete continuous.\n",
    "\n",
    "Number of hidden states grow exponentially with a number of features that are modelled. That is, if the model of a person speech has N states and you need to additionaly include information whether the person is male or female, you now need 2N states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools as ft\n",
    "import itertools as it\n",
    "import json\n",
    "import math\n",
    "import operator as op\n",
    "import os\n",
    "import random\n",
    "\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interact_manual, widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import misc, stats\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete Observations\n",
    "\n",
    "First I will consider a simpler case where hidden states and observations are discrete. Markov model is defined by choosing a number of hidden features $h$ and visible observations $v$ (we don't care about assigning symbols to them and will use numbers $0 \\dots h$ and $0 \\dots v$ as states. It's then necessary to define two matrices of probabilities.\n",
    "\n",
    "First is a matrix of probabilities of transitions between hidden states $H$ of size $h \\times h$ where $h$ is a number of hidden states. $H_{i, j}$ is probability of going from hidden state $i$ to hidden state $j$, for all $i$ $\\sum_{j=0}^{h-1} H_{i, j} = 1$ and for all $i, j$ $H_{i, j} > 0$.\n",
    "\n",
    "Second matrix is a matrix of probabilities of emitting observations while being in a certain state $V$ of size $h \\times v$ where $v$ is a number of possible observations (visible states). $V_{i, k}$ is a probability of emitting observation $k$ while in hidden state $i$. For all $i$ $\\sum_{k=0}^{v-1} V_{i, k} = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  1.  0.]\n",
      "[ 0.25  0.25  0.25  0.25]\n",
      "[ 0.1  0.1  0.7  0.1]\n"
     ]
    }
   ],
   "source": [
    "hidden_n = 4\n",
    "visible_n = 3\n",
    "\n",
    "# square matrix h x h, each row sums to 1\n",
    "hidden_weights = np.array([[0.1, 0.4, 0.4, 0.1], \n",
    "                           [0.2, 0.2, 0.2, 0.4], \n",
    "                           [0.4, 0.2, 0.3, 0.1], \n",
    "                           [0.3, 0.3, 0.1, 0.3]]) \n",
    "# matrix h x v, each row sums to 1\n",
    "visible_weights = np.array([[0.3, 0.4, 0.3], \n",
    "                            [0.1, 0.9, 0.0], \n",
    "                            [0.5, 0.4, 0.1],\n",
    "                            [0.25, 0.1, 0.65]]) \n",
    "\n",
    "def hidden_selected(hidden_n, index):\n",
    "    hidden_states = np.zeros(hidden_n, dtype=np.float64)\n",
    "    hidden_states[index] = 1.0\n",
    "    return hidden_states\n",
    "\n",
    "print(hidden_selected(hidden_n, 2))\n",
    "\n",
    "def hidden_uniform(hidden_n):\n",
    "    return np.ones(hidden_n, dtype=np.float64) / hidden_n\n",
    "\n",
    "print(hidden_uniform(hidden_n))\n",
    "\n",
    "initial_hidden = np.array([0.1, 0.1, 0.7, 0.1])\n",
    "print(initial_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.4  0.2  0.3  0.1]\n",
      "[ 0.25  0.25  0.25  0.25]\n",
      "[ 0.34  0.22  0.28  0.16]\n"
     ]
    }
   ],
   "source": [
    "def generate_visible(visible_weights, hidden_state):\n",
    "    return hidden_weights @ hidden_state\n",
    "\n",
    "print(generate_visible(visible_weights, hidden_selected(hidden_n, 2)))\n",
    "print(generate_visible(visible_weights, hidden_uniform(hidden_n)))\n",
    "print(generate_visible(visible_weights, initial_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (1, 0), (1, 1), (2, 3), (1, 0), (1, 1), (2, 3), (1, 0), (1, 1), (2, 3), (1, 0), (1, 1), (2, 3), (1, 0), (1, 1), (2, 3), (1, 0), (1, 1), (2, 3), (1, 0), (1, 1), (2, 3), (1, 0), (1, 1), (2, 3)]\n",
      "[(2, 3), (2, 3), (1, 1), (0, 2), (1, 2), (0, 2), (0, 0), (0, 2), (1, 1), (0, 3), (2, 0), (0, 2), (0, 2), (1, 1), (2, 2), (2, 2), (1, 1), (0, 0), (1, 1), (0, 2), (2, 2), (1, 1), (2, 3), (1, 1), (2, 3)]\n"
     ]
    }
   ],
   "source": [
    "def walk_max_likely(hidden_weights, visible_weights, initial_hidden):\n",
    "    max_hidden_per_hidden = np.argmax(hidden_weights, axis=1)\n",
    "    max_visible_per_hidden = np.argmax(visible_weights, axis=1)\n",
    "    \n",
    "    hidden_max = np.argmax(initial_hidden)\n",
    "    while True:\n",
    "        visible_max = max_visible_per_hidden[hidden_max]\n",
    "        yield visible_max, hidden_max\n",
    "        hidden_max = max_hidden_per_hidden[hidden_max]\n",
    "        \n",
    "walk = walk_max_likely(hidden_weights, visible_weights, initial_hidden)\n",
    "print(list(it.islice(walk, 25)))\n",
    "\n",
    "\n",
    "def walk_random(hidden_weights, visible_weights, initial_hidden):\n",
    "    hidden_range = np.arange(hidden_weights.shape[1])\n",
    "    visible_range = np.arange(visible_weights.shape[1])\n",
    "    \n",
    "    hidden_chosen = np.random.choice(hidden_range, p=initial_hidden)\n",
    "    while True:\n",
    "        visible_chosen = np.random.choice(visible_range, p=visible_weights[hidden_chosen, :])\n",
    "        yield visible_chosen, hidden_chosen\n",
    "        hidden_chosen = np.random.choice(hidden_range, p=hidden_weights[hidden_chosen, :])\n",
    "        \n",
    "walk = walk_random(hidden_weights, visible_weights, initial_hidden)\n",
    "print(list(it.islice(walk, 25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Algorithm\n",
    "\n",
    "Wiki articles on this are actually quite long and good: [wiki/Baum-Welch](https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm), [wiki/Forward algorithm](https://en.wikipedia.org/wiki/Forward_algorithm).\n",
    "\n",
    "Calculate probability of a given state given that a certain sequence of observations occured in the past.\n",
    "\n",
    "To be exact it calculates $P(x_t, y_t \\dots y_1)$. It does so with recursion \n",
    "\n",
    "$$P(x_t, y_t \\dots y_1) \n",
    "= \\sum_{x_{t-1}} P(x_t, x_{t-1}, y_t \\dots y_1) \n",
    "= \\sum_{x_{t-1}} P(y_t | x_t, x_{t-1}, y_{t-1} \\dots y_1) P(x_t | x_{t-1}, y_{t-1} \\dots y_1) P(x_{t-1}, y_{t-1} \\dots y_1)$$\n",
    "\n",
    "Now, thanks to Markov property we can simplify\n",
    "\n",
    "$$P(y_t | x_t, x_{t-1}, y_{t-1} \\dots y_1) = P(y_t | x_t) = V_{x_t, y_t}$$  \n",
    "\n",
    "$$P(x_t | x_{t-1}, y_{t-1} \\dots y_1) = P(x_t | x_{t-1}) = H_{x_{t-1}, x_t}$$\n",
    "\n",
    "And the final equation is\n",
    "\n",
    "$$P(x_t, y_t \\dots y_1) = V_{x_t, y_t} \\sum_{x_{t-1}} H_{x_{t-1}, x_t} P(x_{t-1}, y_{t-1} \\dots y_1) $$\n",
    "\n",
    "Where $P(x_{t-1}, y_{t-1} \\dots y_1)$ can be recursively calculated in the same way until we get to $P(x_0, \\emptyset) = \\pi_1$ and $\\pi$ is a given vector of size $h$ defining the initial state.\n",
    "\n",
    "<!-- or this $P(x_1, y_1) = P(y_1 | x_1) P(x_1) = V_{x_1,y_1} \\pi_{x_1}$ ? -->\n",
    "\n",
    "Using Markov property reduces complexity from $\\theta(nh^n)$ to $\\theta(nh^2)$, where $n$ is a number of observations.\n",
    "\n",
    "I keep all intermediate states in my implementation to visualize the progress better. I don't really plan to make a production grade system here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.56039699457e-07\n",
      "[[  1.00000000e-01   1.00000000e-01   7.00000000e-01   1.00000000e-01]\n",
      " [  1.36000000e-01   2.07000000e-01   1.12000000e-01   1.50000000e-02]\n",
      " [  4.17200000e-02   1.10430000e-01   5.23600000e-02   1.12100000e-02]\n",
      " [  2.02260000e-02   4.73481000e-02   2.22412000e-02   5.69430000e-03]\n",
      " [  8.83879600e-03   2.13448950e-02   9.92072400e-03   2.48942500e-03]\n",
      " [  3.94719028e-03   9.48192273e-03   4.41186284e-03   1.11607375e-03]\n",
      " [  1.75626833e-03   4.22320982e-03   1.96417075e-03   4.96349653e-04]\n",
      " [  7.81936798e-04   1.87999951e-03   8.74414195e-04   2.21023273e-04]\n",
      " [  2.61079872e-04   0.00000000e+00   9.73201207e-05   6.39562225e-04]\n",
      " [  1.02761881e-04   2.84188177e-04   7.90336831e-05   2.27708667e-05]\n",
      " [  4.22234227e-05   1.08522346e-04   4.95718318e-05   1.38686087e-05]\n",
      " [  1.99664507e-05   4.74019085e-05   2.19408995e-05   5.67490465e-06]\n",
      " [  6.58675739e-06   0.00000000e+00   2.46167223e-06   1.61550804e-05]\n",
      " [  1.94696062e-06   0.00000000e+00   4.98871266e-07   3.73838860e-06]\n",
      " [  4.54728345e-07   2.00007508e-07   6.51142245e-07   3.41524942e-07]\n",
      " [  1.79355487e-07   4.09120894e-07   1.80555203e-07   2.93047545e-08]\n",
      " [  7.23092940e-08   1.78621956e-07   8.42653639e-08   2.08430853e-08]]\n",
      "0.046731\n",
      "[[ 0.        1.        0.        0.      ]\n",
      " [ 0.06      0.        0.02      0.26    ]\n",
      " [ 0.0368    0.0954    0.0224    0.0086  ]\n",
      " [ 0.01029   0.004086  0.02069   0.011665]]\n",
      "0.01898925\n",
      "[[ 1.          0.          0.          0.        ]\n",
      " [ 0.03        0.          0.04        0.065     ]\n",
      " [ 0.0154      0.03555     0.0122      0.00265   ]\n",
      " [ 0.0042975   0.0016505   0.0085975   0.00444375]]\n"
     ]
    }
   ],
   "source": [
    "def forward_probability(hidden_weights, visible_weights, initial_hidden, observations):\n",
    "    joint_probs = np.zeros((observations.size + 1, initial_hidden.size), dtype=np.float64)\n",
    "    joint_probs[0, :] = initial_hidden\n",
    "    \n",
    "    for i, observation in enumerate(observations):\n",
    "        joint_probs[i + 1] = (joint_probs[i] @ hidden_weights) * visible_weights[:, observation]\n",
    "            \n",
    "    return joint_probs[-1].sum(), joint_probs\n",
    "    \n",
    "walk = walk_random(hidden_weights, visible_weights, initial_hidden)\n",
    "observations = np.array([v for v, h in it.islice(walk, 16)])\n",
    "print(*forward_probability(hidden_weights, visible_weights, initial_hidden, observations), sep='\\n')\n",
    "\n",
    "observations = np.array([2, 1, 0])\n",
    "print(*forward_probability(hidden_weights, visible_weights, hidden_selected(4, 1), observations), sep='\\n')\n",
    "\n",
    "observations = np.array([2, 1, 0])\n",
    "print(*forward_probability(hidden_weights, visible_weights, hidden_selected(4, 0), observations), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Algorithm\n",
    "\n",
    "It's similar to forward algorithm. It calculates $P(y_{t+1} \\dots y_T | x_t)$ that is probability of future sequence of observations $y_{t+1} \\dots y_T$ given initial state $x_t$ at time $t$. \n",
    "\n",
    "It starts with the final state $x_T$.\n",
    "\n",
    "$$P(y_{t+1} \\dots y_{T} | x_t)\n",
    "= \\sum_{x_{t+1}} P(y_{t+1} \\dots y_{T}, x_{t+1} | x_t)\n",
    "= \\sum_{x_{t+1}} P(y_{t+1} | y_{t+2} \\dots y_{T}, x_{t+1}, x_t) P(y_{t+2} \\dots y_{T} | x_{t+1}, x_t) P(x_{t+1} | x_t)$$\n",
    "\n",
    "Now we can hopefully simplify:\n",
    "\n",
    "$$P(y_{t+1} | y_{t+2} \\dots y_{T}, x_{t+1}, x_t) = P(y_{t+1} | x_{t+1}) = V_{x_{t+1},y_{t+1}}$$\n",
    "\n",
    "$$P(y_{t+2} \\dots y_{T} | x_{t+1}, x_t) = P(y_{t+2} \\dots y_{T} | x_{t+1})$$\n",
    "\n",
    "$$P(x_{t+1} | x_t) = H_{x_t,x_{t+1}}$$\n",
    "\n",
    "And the end result is:\n",
    "\n",
    "$$P(y_{t+1} \\dots y_{T} | x_t) =  \\sum_{x_{t+1}} V_{x_{t+1},y_{t+1}} H_{x_t,x_{t+1}} P(y_{t+2} \\dots y_{T} | x_{t+1})$$\n",
    "\n",
    "The probabilities are computed recursively. Base case is $P(\\emptyset | x_T) = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.32360852066e-07\n",
      "[[  1.44303052e-07   1.26907056e-07   1.51407262e-07   1.09743482e-07]\n",
      " [  4.63201642e-07   4.40443395e-07   5.15501399e-07   3.87559496e-07]\n",
      " [  1.75650861e-06   1.24308285e-06   1.61848723e-06   1.48342499e-06]\n",
      " [  4.16099094e-06   2.75610462e-06   3.53103658e-06   3.29054555e-06]\n",
      " [  7.77487839e-06   6.83753007e-06   8.30460889e-06   5.97475557e-06]\n",
      " [  2.64666440e-05   2.32697110e-05   2.77374648e-05   2.01039074e-05]\n",
      " [  8.46054189e-05   8.06485493e-05   9.46139183e-05   7.11902301e-05]\n",
      " [  3.25613410e-04   2.27982736e-04   2.94741055e-04   2.70779844e-04]\n",
      " [  7.31556909e-04   5.12128753e-04   6.61900222e-04   6.08074760e-04]\n",
      " [  1.64103516e-03   1.15025128e-03   1.48839188e-03   1.36823397e-03]\n",
      " [  3.71326108e-03   2.58607620e-03   3.31852224e-03   3.05537255e-03]\n",
      " [  8.00821555e-03   5.83637700e-03   7.62942115e-03   7.11293505e-03]\n",
      " [  2.11782050e-02   1.30002200e-02   1.45034850e-02   1.60450550e-02]\n",
      " [  1.82995000e-02   4.28100000e-02   2.91215000e-02   3.75185000e-02]\n",
      " [  1.37800000e-01   1.34800000e-01   1.58000000e-01   1.20700000e-01]\n",
      " [  5.70000000e-01   3.80000000e-01   4.70000000e-01   4.60000000e-01]\n",
      " [  1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00]]\n",
      "0.013276849375\n",
      "[[ 0.00173233  0.00512761  0.00229158  0.00412533]\n",
      " [ 0.00729638  0.0214205   0.00974238  0.01728838]\n",
      " [ 0.031825    0.0891      0.041825    0.071825  ]\n",
      " [ 0.135       0.34        0.215       0.295     ]\n",
      " [ 1.          1.          1.          1.        ]]\n",
      "0.4721\n",
      "[[ 0.4096  0.      0.0625]\n",
      " [ 0.512   0.      0.125 ]\n",
      " [ 0.64    0.      0.25  ]\n",
      " [ 0.8     0.      0.5   ]\n",
      " [ 1.      1.      1.    ]]\n"
     ]
    }
   ],
   "source": [
    "def backward_probability(hidden_weights, visible_weights, observations):\n",
    "    conditional_probs = np.zeros((observations.size + 1, hidden_weights.shape[0]), dtype=np.float64)\n",
    "    conditional_probs[-1, :] = 1\n",
    "    \n",
    "    for i, observation in reversed(list(enumerate(observations))):\n",
    "        conditional_probs[i] = (conditional_probs[i + 1] * visible_weights[:, observation]) @ hidden_weights.T\n",
    "            \n",
    "    return conditional_probs[0].sum(), conditional_probs\n",
    "    \n",
    "walk = walk_random(hidden_weights, visible_weights, initial_hidden)\n",
    "observations = np.array([v for v, h in it.islice(walk, 16)])\n",
    "print(*backward_probability(hidden_weights, visible_weights, observations), sep='\\n')\n",
    "\n",
    "observations = np.array([2, 2, 2, 2])\n",
    "print(*backward_probability(hidden_weights, visible_weights, observations), sep='\\n')\n",
    "\n",
    "observations = np.array([1, 1, 1, 1])\n",
    "print(*backward_probability(np.eye(3), np.array([[0.2, 0.8], [1.0, 0.0], [0.5, 0.5]]), observations), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward-Backward Algorithm\n",
    "\n",
    "For any $k = 1 \\dots t$\n",
    "\n",
    "* Forward algorithm calculates $P(x_t, y_t \\dots y_1)$. It calculates probability of a hidden state and a sequence of past observations at time $t$.\n",
    "* Backward algorithm calculates $P(y_T \\dots y_{t+1} | x_t)$. It calculates probability of a sequence of future observations $y_{t+1} \\dots y_T$ given a current state $x_t$ at time $t$.\n",
    "* Forward-backward algorithm calculates $P(x_t | y_T \\dots y_1)$. So it gives probability of any hidden state $x_t$ at any time taking into accout full sequence of observations. It can then at any time select the most likely state. Note that by selecting the most likely state we will not necessarily get the most likely sequence. Viterbi algorithms is needed for that.\n",
    "\n",
    "Forward-backward algorithm combines the results of the previous two algorithms.\n",
    "\n",
    "$$P(x_t, y_t \\dots y_1) P(y_T \\dots y_{t+1} | x_t) \n",
    "= P(y_t \\dots y_1 | x_t) P(y_T \\dots y_{t+1} | x_t) P(x_t) \n",
    "= P(y_T \\dots y_1 | x_t) P(x_t) \n",
    "= P(y_T \\dots y_1, x_t) \n",
    "= P(x_t | y_T \\dots y_1) P(y_T \\dots y_1)$$\n",
    "\n",
    "$$P(x_t | y_T \\dots y_1) = \\frac{P(x_t, y_t \\dots y_1) P(y_T \\dots y_{t+1} | x_t)}{P(y_T \\dots y_1)}$$\n",
    "\n",
    "$P(y_T \\dots y_1)$ can be calculated as a marginal distribution of $\\sum_{x_T} P(x_T, y_T \\dots y_1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10186777  0.0896863   0.73204128  0.07640464]\n",
      " [ 0.3085618   0.07162325  0.51207468  0.10774027]\n",
      " [ 0.35582238  0.07491621  0.44794118  0.12132024]\n",
      " [ 0.12190915  0.62833388  0.21392863  0.03582834]\n",
      " [ 0.31029617  0.          0.10113975  0.58856408]\n",
      " [ 0.21377889  0.56187313  0.18477054  0.03957744]\n",
      " [ 0.25325211  0.06750127  0.44891593  0.23033068]\n",
      " [ 0.29361925  0.42869215  0.24106035  0.03662825]\n",
      " [ 0.2410574   0.43381504  0.26596914  0.05915842]\n",
      " [ 0.27110046  0.42104765  0.2481342   0.0597177 ]\n",
      " [ 0.2062698   0.48005598  0.2635385   0.05013572]\n",
      " [ 0.28070503  0.06618658  0.44342586  0.20968253]\n",
      " [ 0.25175911  0.47043376  0.24918719  0.02861994]\n",
      " [ 0.23282503  0.08033678  0.5022377   0.18460049]\n",
      " [ 0.32057354  0.09561011  0.45672954  0.12708682]\n",
      " [ 0.17694599  0.14702262  0.49377463  0.18225676]\n",
      " [ 0.3964654   0.          0.13615381  0.46738079]]\n",
      "[[  5.26025456e-09   4.63122702e-09   3.78011937e-08   3.94538757e-09]\n",
      " [  1.59335335e-08   3.69848583e-09   2.64425446e-08   5.56349884e-09]\n",
      " [  1.83739782e-08   3.86852781e-09   2.31308146e-08   6.26474217e-09]\n",
      " [  6.29515243e-09   3.24459444e-08   1.10468599e-08   1.85010610e-09]\n",
      " [  1.60230931e-08   0.00000000e+00   5.22266085e-09   3.03923089e-08]\n",
      " [  1.10391277e-08   2.90140401e-08   9.54119272e-09   2.04370234e-09]\n",
      " [  1.30774485e-08   3.48563484e-09   2.31811492e-08   1.18938303e-08]\n",
      " [  1.51619293e-08   2.21368319e-08   1.24478896e-08   1.89141194e-09]\n",
      " [  1.24477373e-08   2.24013685e-08   1.37341309e-08   3.05482612e-09]\n",
      " [  1.39991025e-08   2.17420849e-08   1.28131692e-08   3.08370625e-09]\n",
      " [  1.06513727e-08   2.47891610e-08   1.36086175e-08   2.58891161e-09]\n",
      " [  1.44950640e-08   3.41774689e-09   2.28976524e-08   1.08275995e-08]\n",
      " [  1.30003529e-08   2.42922880e-08   1.28675437e-08   1.47787823e-09]\n",
      " [  1.20226337e-08   4.14843573e-09   2.59345819e-08   9.53241156e-09]\n",
      " [  1.65537964e-08   4.93712064e-09   2.35846285e-08   6.56251722e-09]\n",
      " [  9.13714816e-09   7.59196337e-09   2.54975652e-08   9.41138605e-09]\n",
      " [  2.04727051e-08   0.00000000e+00   7.03071907e-09   2.41346386e-08]]\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "\n",
      "[[ 0.          1.          0.          0.        ]\n",
      " [ 0.21242858  0.          0.05899724  0.72857418]\n",
      " [ 0.23230832  0.57161199  0.15099185  0.04508784]\n",
      " [ 0.22019644  0.08743661  0.44274678  0.24962017]]\n",
      "[[ 0.        0.046731  0.        0.      ]\n",
      " [ 0.009927  0.        0.002757  0.034047]\n",
      " [ 0.010856  0.026712  0.007056  0.002107]\n",
      " [ 0.01029   0.004086  0.02069   0.011665]]\n",
      "\n",
      "[[ 1.          0.          0.          0.        ]\n",
      " [ 0.26138473  0.          0.29037482  0.44824045]\n",
      " [ 0.23924062  0.52419132  0.20237766  0.0341904 ]\n",
      " [ 0.22631226  0.0869176   0.45275616  0.23401398]]\n",
      "[[ 0.01898925  0.          0.          0.        ]\n",
      " [ 0.0049635   0.          0.005514    0.00851175]\n",
      " [ 0.004543    0.009954    0.003843    0.00064925]\n",
      " [ 0.0042975   0.0016505   0.0085975   0.00444375]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def forward_backward_probability(hidden_weights, visible_weights, initial_hidden, observations):\n",
    "    _, forward_result = forward_probability(hidden_weights, visible_weights, initial_hidden, observations)\n",
    "    _, backward_result = backward_probability(hidden_weights, visible_weights, observations)\n",
    "    scaling_coefficient = forward_result[-1, :].sum()\n",
    "    combined_result = forward_result * backward_result\n",
    "    return combined_result / scaling_coefficient, combined_result\n",
    "\n",
    "walk = walk_random(hidden_weights, visible_weights, initial_hidden)\n",
    "observations = np.array([v for v, h in it.islice(walk, 16)])\n",
    "print(*forward_backward_probability(hidden_weights, visible_weights, initial_hidden, observations), sep='\\n')\n",
    "print(forward_backward_probability(hidden_weights, visible_weights, initial_hidden, observations)[0].sum(axis=1))\n",
    "print()\n",
    "\n",
    "observations = np.array([2, 1, 0])\n",
    "print(*forward_backward_probability(hidden_weights, visible_weights, hidden_selected(4, 1), observations), sep='\\n')\n",
    "print()\n",
    "\n",
    "observations = np.array([2, 1, 0])\n",
    "print(*forward_backward_probability(hidden_weights, visible_weights, hidden_selected(4, 0), observations), sep='\\n')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baum-Welch Algorithm\n",
    "\n",
    "Estimate the most likely weights $H$, $V$ and $\\pi$ (initial state) of a model given sequences of observations.\n",
    "\n",
    "It fits the weights using Expectation Maximization. The forward-backward algorithm is used to find the likelihoods of hidden states given a sequence of observations.\n",
    "\n",
    "We proved previously that forward-backward algorithm can calculate $P(x_t | y_T \\dots y_1)$ as:\n",
    "\n",
    "$$P(x_t | y_T \\dots y_1) = \\frac{P(x_t, y_t \\dots y_1) P(y_T \\dots y_{t+1} | x_t)}{P(y_T \\dots y_1)}$$\n",
    "\n",
    "where $P(x_t, y_t \\dots y_1)$ $P(y_T \\dots y_{t+1} | x_t)$ are given to us by forward and backward algorithms and $P(y_T \\dots y_1)$ can be calculated as a marginal distribution of $\\sum_{x_T} P(x_T, y_T \\dots y_1)$\n",
    "\n",
    "To perform Baum-Welch learning we additionally need $P(x_t, x_{t+1} | y_T \\dots y_1)$, which is the probability of transitioning from state $x_t$ at time $t$ to state $x_{t+1}$ at time $t+1$ given the sequence of observations. (and implicitly given model parameters)\n",
    "\n",
    "$$P(x_t, x_{t+1} | y_T \\dots y_1)\n",
    "= \\frac{P(x_t, x_{t+1}, y_T \\dots y_1)}{P(y_T \\dots y_1)}$$\n",
    "\n",
    "$$P(x_t, x_{t+1}, y_T \\dots y_1) \\\\\n",
    "= P(y_T \\dots y_1 | x_t, x_{t+1}) P(x_{t+1} | x_t) P(x_t) \\\\\n",
    "= P(y_T \\dots y_{t+2} | x_t, x_{t+1}) P(y_{t+1} \\dots y_1 | x_t, x_{t+1}) P(x_{t+1} | x_t) P(x_t)$$\n",
    "\n",
    "$$P(y_{t+1} \\dots y_1 | x_t, x_{t+1}) P(x_t) \n",
    "= P(y_{t+1} \\dots y_1, x_t | x_{t+1})\n",
    "= P(y_{t+1} | x_{t+1}) P(y_t \\dots y_1, x_t | x_{t+1})$$\n",
    "\n",
    "$$P(x_t, x_{t+1}, y_T \\dots y_1) \\\\\n",
    "= P(y_T \\dots y_{t+2} | x_t, x_{t+1}) P(y_{t+1} | x_{t+1}) P(y_t \\dots y_1, x_t | x_{t+1}) P(x_{t+1} | x_t) \\\\\n",
    "= P(y_T \\dots y_{t+2} | x_{t+1}) P(y_{t+1} | x_{t+1}) P(y_t \\dots y_1, x_t) P(x_{t+1} | x_t)$$\n",
    "\n",
    "$$P(x_t, x_{t+1} | y_T \\dots y_1)\n",
    "= \\frac{P(y_T \\dots y_{t+2} | x_{t+1}) P(y_t \\dots y_1, x_t) V_{x_{t+1}, y_{t+1}} H_{x_t, x_{t+1}}}{P(y_T \\dots y_1)}$$\n",
    "\n",
    "Then the updates are performed like this:\n",
    "\n",
    "$$\\pi_i = P(X_0 = x_i | y_T \\dots y_1)$$\n",
    "\n",
    "$$H_{x_t, x_{t+1}} = \\frac{\\sum_{t=0}^{T-1} P(x_t, x_{t+1} | y_T \\dots y_1)}{\\sum_{t=0}^{T-1} P(x_t | y_T \\dots y_1)}$$\n",
    "\n",
    "$$V_{x_t, y_t} = \\frac{\\sum_{t=1}^T \\mathbb{1}_{Y_t = y_t} P(x_t | y_T \\dots y_1)}{\\sum_{t=1}^T P(x_t | y_T \\dots y_1)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.14040492  0.08595171  0.668161    0.10548237]\n",
      " [ 0.15600941  0.58415643  0.22183497  0.03799919]\n",
      " [ 0.31849614  0.          0.10806445  0.57343941]\n",
      " [ 0.19776219  0.57147756  0.1890466   0.04171365]\n",
      " [ 0.29680417  0.06860147  0.38315739  0.25143697]\n",
      " [ 0.12038918  0.69631166  0.14293509  0.04036406]\n",
      " [ 0.13867821  0.          0.09377518  0.7675466 ]\n",
      " [ 0.3609491   0.          0.07663579  0.56241511]\n",
      " [ 0.20857963  0.59138198  0.16305308  0.03698531]\n",
      " [ 0.1266559   0.12117468  0.43293492  0.31923449]\n",
      " [ 0.45065895  0.          0.09081833  0.45852272]\n",
      " [ 0.08775434  0.75308722  0.11766254  0.04149589]\n",
      " [ 0.13798243  0.          0.08382133  0.77819624]\n",
      " [ 0.3452775   0.          0.09381099  0.56091151]\n",
      " [ 0.30583238  0.09572094  0.40207285  0.19637382]\n",
      " [ 0.28699911  0.43690158  0.23816812  0.03793118]\n",
      " [ 0.20309317  0.50169326  0.23664768  0.05856589]]\n",
      "[[ 0.37577458  0.64826055  1.31255359  1.40533311]\n",
      " [ 1.56798731  0.43114453  0.95368881  1.46768613]\n",
      " [ 1.25632842  0.62903524  0.81551836  0.3735573 ]\n",
      " [ 0.47914328  2.29632491  0.42419189  1.56347199]]\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "[ 3.74192183  4.42050678  3.07443933  4.76313206]\n",
      "\n",
      "new initial\n",
      "[ 0.14040492  0.08595171  0.668161    0.10548237]\n",
      "1.0\n",
      "new hidden\n",
      "[[ 0.10042288  0.17324268  0.35076991  0.37556453]\n",
      " [ 0.35470759  0.09753283  0.21574196  0.33201762]\n",
      " [ 0.4086366   0.20460161  0.26525759  0.1215042 ]\n",
      " [ 0.10059416  0.48210398  0.08905734  0.32824452]]\n",
      "[ 0.96436123  0.95748109  0.92082681  1.15733087]\n",
      "[ 1.  1.  1.  1.]\n",
      "new visible\n",
      "[[ 0.19489783  0.33688225  0.46821992]\n",
      " [ 0.0645847   0.9354153   0.        ]\n",
      " [ 0.39622352  0.42588191  0.17789457]\n",
      " [ 0.161038    0.06194562  0.77701637]]\n",
      "[ 0.81674405  1.76012508  1.42313086]\n",
      "[ 1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "def forward_backward_baum_welch(hidden_weights, visible_weights, initial_hidden, observations):\n",
    "    _, forward_result = forward_probability(hidden_weights, visible_weights, initial_hidden, observations)\n",
    "    _, backward_result = backward_probability(hidden_weights, visible_weights, observations)\n",
    "    scaling_coefficient = forward_result[-1, :].sum()\n",
    "    \n",
    "    per_state_result = forward_result * backward_result # observation+1 x state\n",
    "    per_state_result /= scaling_coefficient\n",
    "    \n",
    "    # state x state (we can sum over time/observations)\n",
    "#     per_pair_result = (forward_result[:-1].T @ (backward_result[1:] * visible_weights[:, observations].T)) * hidden_weights\n",
    "    per_pair_result = ((backward_result[1:] * visible_weights[:, observations].T).T @ forward_result[:-1]) * hidden_weights.T\n",
    "    per_pair_result /= scaling_coefficient\n",
    "    \n",
    "    return per_state_result, per_pair_result\n",
    "\n",
    "def estimate_params_baum_welch(per_state_result, per_pair_result, observations, visible_n):\n",
    "    hidden_n = per_pair_result.shape[0]\n",
    "    \n",
    "    new_initial_hidden = per_state_result[0]\n",
    "    \n",
    "    per_state_rest = per_state_result[1:]\n",
    "    per_state_rest_total = per_state_rest.sum(axis=0)\n",
    "    \n",
    "    new_hidden_weights = per_pair_result / per_state_rest_total[:, np.newaxis]\n",
    "    \n",
    "    new_visible_weights = np.zeros((hidden_n, visible_n), dtype=np.float64)\n",
    "    for i in range(visible_n):\n",
    "        new_visible_weights[:, i] = per_state_rest[observations == i].sum(axis=0) / per_state_rest_total\n",
    "        \n",
    "    return new_hidden_weights, new_visible_weights, new_initial_hidden\n",
    "\n",
    "walk = walk_random(hidden_weights, visible_weights, initial_hidden)\n",
    "observations = np.array([v for v, h in it.islice(walk, 16)])\n",
    "result = forward_backward_baum_welch(hidden_weights, visible_weights, initial_hidden, observations)\n",
    "print(*result, result[0].sum(axis=1), result[1].sum(axis=1), sep='\\n')\n",
    "print()\n",
    "\n",
    "new_hidden_weights, new_visible_weights, new_initial_hidden = \\\n",
    "    estimate_params_baum_welch(*result, observations, visible_weights.shape[1])\n",
    "\n",
    "print('new initial', new_initial_hidden, new_initial_hidden.sum(), sep='\\n')\n",
    "print('new hidden', new_hidden_weights, new_hidden_weights.sum(axis=0), new_hidden_weights.sum(axis=1), sep='\\n')\n",
    "print('new visible', new_visible_weights, new_visible_weights.sum(axis=0), new_visible_weights.sum(axis=1), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouper(iterable, n):\n",
    "    args = [iter(iterable)] * n\n",
    "    return zip(*args)\n",
    "\n",
    "def estimate_weights_baum_welch(hidden_n, visible_n, observations_batch, iterations, batch_n=None, \n",
    "                                hidden_weights=None, visible_weights=None, initial_hidden=None):\n",
    "    if hidden_weights is None:\n",
    "        hidden_weights = np.random.uniform(size=(hidden_n, hidden_n))\n",
    "        hidden_weights /= hidden_weights.sum(axis=1)[:, np.newaxis]\n",
    "    if visible_weights is None:\n",
    "        visible_weights = np.random.uniform(size=(hidden_n, visible_n))\n",
    "        visible_weights /= visible_weights.sum(axis=1)[:, np.newaxis]\n",
    "    if initial_hidden is None:\n",
    "        initial_hidden = np.random.uniform(size=hidden_n)\n",
    "        initial_hidden /= initial_hidden.sum()\n",
    "    batch_n = batch_n or len(observations_batch)\n",
    "        \n",
    "    batch_hidden_weights = np.zeros((batch_n, hidden_n, hidden_n), dtype=np.float64)\n",
    "    batch_visible_weights = np.zeros((batch_n, hidden_n, visible_n), dtype=np.float64)\n",
    "    batch_initial_hidden = np.zeros((batch_n, hidden_n), dtype=np.float64)\n",
    "    shuffled_observations = observations_batch[:]\n",
    "    for i in range(iterations):\n",
    "        random.shuffle(shuffled_observations)\n",
    "        observation_groups = grouper(shuffled_observations, batch_n)\n",
    "        for observation_group in observation_groups:\n",
    "            for j, observations in enumerate(observation_group):\n",
    "                per_state_result, per_pair_result = \\\n",
    "                    forward_backward_baum_welch(hidden_weights, visible_weights, initial_hidden, observations)\n",
    "                batch_hidden_weights[j], batch_visible_weights[j], batch_initial_hidden[j] = \\\n",
    "                    estimate_params_baum_welch(per_state_result, per_pair_result, observations, visible_n)\n",
    "            hidden_weights = batch_hidden_weights.mean(axis=0) \n",
    "            visible_weights = batch_visible_weights.mean(axis=0) \n",
    "            initial_hidden = batch_initial_hidden.mean(axis=0)\n",
    "            \n",
    "    return hidden_weights, visible_weights, initial_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def test_baum_welch(hidden_weights, visible_weights, initial_hidden, observations_n, \n",
    "                    observations_min, observations_max, iterations, bw_kwargs):\n",
    "    observations_batch = []\n",
    "    for i in range(observations_n):\n",
    "        walk = walk_random(hidden_weights, visible_weights, initial_hidden)\n",
    "        length = int(np.random.uniform(observations_min, observations_max))\n",
    "        observations = np.array([v for v, h in it.islice(walk, length)])\n",
    "        observations_batch.append(observations)\n",
    "\n",
    "    print('Generator', hidden_weights, visible_weights, initial_hidden, sep='\\n', end='\\n\\n')\n",
    "\n",
    "    hidden_n, visible_n = visible_weights.shape\n",
    "    result = estimate_weights_baum_welch(hidden_n, visible_n, observations_batch, \n",
    "                                         iterations=iterations, **bw_kwargs)\n",
    "\n",
    "    print('Result', *result, sep='\\n', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator\n",
      "[[ 0.1  0.4  0.4  0.1]\n",
      " [ 0.2  0.2  0.2  0.4]\n",
      " [ 0.4  0.2  0.3  0.1]\n",
      " [ 0.3  0.3  0.1  0.3]]\n",
      "[[ 0.3   0.4   0.3 ]\n",
      " [ 0.1   0.9   0.  ]\n",
      " [ 0.5   0.4   0.1 ]\n",
      " [ 0.25  0.1   0.65]]\n",
      "[ 0.1  0.1  0.7  0.1]\n",
      "\n",
      "Result\n",
      "[[  1.00000000e+000   5.22252755e-028   2.66742904e-048   6.90050353e-036]\n",
      " [  1.00000000e+000   0.00000000e+000   1.94516426e-158   0.00000000e+000]\n",
      " [  1.00000000e+000   1.28416598e-058   0.00000000e+000   3.90954505e-138]\n",
      " [  1.00000000e+000   0.00000000e+000   5.34401269e-255   0.00000000e+000]]\n",
      "[[  2.00000000e-01   5.33333333e-01   2.66666667e-01]\n",
      " [  8.63652144e-05   5.29693418e-10   9.99913634e-01]\n",
      " [  7.24733460e-12   1.00000000e+00   2.11146260e-24]\n",
      " [  7.15078027e-06   4.30829374e-10   9.99992849e-01]]\n",
      "[  1.00000000e+00   3.92835005e-28   2.84166719e-48   5.19056198e-36]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_baum_welch(hidden_weights, visible_weights, initial_hidden, 100, 30, 40, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator\n",
      "[[ 0.1  0.4  0.4  0.1]\n",
      " [ 0.2  0.2  0.2  0.4]\n",
      " [ 0.4  0.2  0.3  0.1]\n",
      " [ 0.3  0.3  0.1  0.3]]\n",
      "[[ 0.3   0.4   0.3 ]\n",
      " [ 0.1   0.9   0.  ]\n",
      " [ 0.5   0.4   0.1 ]\n",
      " [ 0.25  0.1   0.65]]\n",
      "[ 0.1  0.1  0.7  0.1]\n",
      "\n",
      "Result\n",
      "[[  0.00000000e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00]\n",
      " [  1.06049260e-06   3.65523980e-03   9.95777959e-01   5.65740293e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00]]\n",
      "[[  3.55237139e-18   1.00000000e+00   3.65452186e-17]\n",
      " [  7.03419078e-16   1.00000000e+00   5.43795357e-16]\n",
      " [  3.88853436e-01   2.87102034e-01   3.24044530e-01]\n",
      " [  1.68538904e-16   1.00000000e+00   5.98485487e-17]]\n",
      "[  1.43583779e-06   4.94895618e-03   9.94283633e-01   7.65975442e-04]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_baum_welch(hidden_weights, visible_weights, initial_hidden, 100, 30, 40, 100, bw_kwargs={'batch_n': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator\n",
      "[[ 0.1  0.4  0.4  0.1]\n",
      " [ 0.2  0.2  0.2  0.4]\n",
      " [ 0.4  0.2  0.3  0.1]\n",
      " [ 0.3  0.3  0.1  0.3]]\n",
      "[[ 0.3   0.4   0.3 ]\n",
      " [ 0.1   0.9   0.  ]\n",
      " [ 0.5   0.4   0.1 ]\n",
      " [ 0.25  0.1   0.65]]\n",
      "[ 0.1  0.1  0.7  0.1]\n",
      "\n",
      "Result\n",
      "[[ 0.08757035  0.32202782  0.44420604  0.14619579]\n",
      " [ 0.15190219  0.08815936  0.23845553  0.52148291]\n",
      " [ 0.30941591  0.19746626  0.35763381  0.13548401]\n",
      " [ 0.29253774  0.32330515  0.11388265  0.27027446]]\n",
      "[[ 0.33949107  0.4594282   0.20108072]\n",
      " [ 0.03822443  0.96177557  0.        ]\n",
      " [ 0.46101205  0.44646156  0.09252639]\n",
      " [ 0.25720772  0.13611007  0.6066822 ]]\n",
      "[  1.91884517e-02   4.00181709e-09   1.16793406e-01   8.64018139e-01]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_baum_welch(hidden_weights, visible_weights, initial_hidden, 10, 300, 400, 10, \n",
    "                bw_kwargs={'batch_n': 1,\n",
    "                           'hidden_weights': hidden_weights, \n",
    "                           'visible_weights': visible_weights, \n",
    "                           'initial_hidden': initial_hidden})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator\n",
      "[[ 0.5  0.5]\n",
      " [ 0.5  0.5]]\n",
      "[[ 0.5  0.5]\n",
      " [ 0.5  0.5]]\n",
      "[ 0.5  0.5]\n",
      "\n",
      "Result\n",
      "[[ 0.93749168  0.06250832]\n",
      " [ 0.94778407  0.05221593]]\n",
      "[[ 0.46543507  0.53456493]\n",
      " [ 0.57031706  0.42968294]]\n",
      "[ 0.93771769  0.06228231]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_baum_welch(np.array([[0.5, 0.5], [0.5, 0.5]]), \n",
    "                np.array([[0.5, 0.5], [0.5, 0.5]]), \n",
    "                np.array([0.5, 0.5]),\n",
    "                50, 50, 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator\n",
      "[[ 0.9  0.1]\n",
      " [ 0.1  0.9]]\n",
      "[[ 1.  0.]\n",
      " [ 0.  1.]]\n",
      "[ 0.5  0.5]\n",
      "\n",
      "Result\n",
      "[[ 0.91131163  0.08868837]\n",
      " [ 0.09903204  0.90096796]]\n",
      "[[ 0.99875008  0.00124992]\n",
      " [ 0.00479634  0.99520366]]\n",
      "[ 0.49701092  0.50298908]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_baum_welch(np.array([[0.9, 0.1], [0.1, 0.9]]), \n",
    "                np.eye(2), \n",
    "                np.array([0.5, 0.5]),\n",
    "                6, 400, 500, 100, bw_kwargs={'batch_n': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems not to work very well. Well then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi Algorithm\n",
    "\n",
    "https://www.vocal.com/echo-cancellation/viterbi-algorithm-in-speech-enhancement-and-hmm/\n",
    "\n",
    "Calculate the most likely sequence of hidden states given a sequence of observations.\n",
    "\n",
    "Using forward-backward algorithm we calculated probabilities for all states at all times given a sequence of observations P(x_t | y_T \\dots y_1). We could just take the most probable state at each time but it would not necessarily give us the most probable sequence. We could select two states that were most likely separately, but that are very unlikely to occur together because the probability of transitioning from the first one to the second is very low or impossible (the corresponding probability in matrix $H$ is very low). \n",
    "\n",
    "Viterbi algorithm solves problem of finding a sequence $x_T \\dots x_1$ that maximizes $P(x_T \\dots x_1 | y_T \\dots y_1)$. \n",
    "\n",
    "It uses a dynamic programming technique. It builds two matrices, let's call them $A$ and $B$. \n",
    "\n",
    "$A_{t,x_i} = max_{x_1 \\dots x_{t-1}} P(x_1 \\dots x_{t-1}, X_t = x_i, y_1 \\dots y_t)$ so it contains the highest probability of single sequence of states $x_1 \\dots x_{t-1}$ occuring and ending in state $x_i$ joint with a sequence of observations $y_1 \\dots y_t$ occuring.\n",
    "\n",
    "$B_{t,x_i} = argmax_{x_1 \\dots x_{t-1}} P(x_1 \\dots x_{t-1}, X_t = x_i, y_1 \\dots y_t)$ and it is used to record\n",
    "which states were chosen as the states of maximum probability while calculating $A$. It is then used to recover the most likely path at the end. Note that $t > 1$, because at $t = 1$ we do not choose anything while calculating values in $A$.\n",
    "\n",
    "Given A_{t,x_i} we can recursively calculate probabilities for next time steps A_{t+1,x_i} as follows:\n",
    "\n",
    "$$P(x_1 \\dots x_t, y_1 \\dots y_t) \\\\\n",
    "= P(y_t | x_1 \\dots x_t, y_1 \\dots y_{t-1}) P(x_1 \\dots x_t, y_1 \\dots y_{t-1}) \\\\\n",
    "= P(y_t | x_1 \\dots x_t, y_1 \\dots y_{t-1}) P(x_t | x_1 \\dots x_{t-1}, y_1 \\dots y_{t-1}) P(x_1 \\dots x_{t-1}, y_1 \\dots y_{t-1}) \\\\\n",
    "= P(y_t | x_t) P(x_t | x_{t-1}) P(x_1 \\dots x_{t-1}, y_1 \\dots y_{t-1}) \\\\\n",
    "= V_{x_t, y_t} H_{x_{t-1}, x_t} P(x_1 \\dots x_{t-1}, y_1 \\dots y_{t-1})$$\n",
    "\n",
    "$$A_{t,x_i} \\\\\n",
    "= max_{x_1 \\dots x_{t-1}} P(x_1 \\dots x_{t-1}, X_t = x_i, y_1 \\dots y_t) \\\\\n",
    "= max_{x_1 \\dots x_{t-1}} V_{x_i, y_t} H_{x_{t-1}, x_i} P(x_1 \\dots x_{t-1}, y_1 \\dots y_{t-1}) \\\\\n",
    "= V_{x_i, y_t} max_{x_{t-1}}(H_{x_{t-1}, x_i} A_{t-1, x_{t-1}})$$\n",
    "\n",
    "Note that the base case of this recursion is $A_{1,x_i} = P(X_1 = x_i, y_1) = \\pi_i V_{x_i, y_1}$\n",
    "\n",
    "Also regarding B \n",
    "$$B_{t, x_i} = argmax_{x_1 \\dots x_{t-1}} P(x_1 \\dots x_{t-1}, X_t = x_i, y_1 \\dots y_t) = argmax_{x_{t-1}} H_{x_{t-1}, x_i} A_{t-1, x_{t-1}}$$\n",
    "\n",
    "After computing whole $A$ we can find the most probable path by selecting the state x_i with highest probability at time T $max_{x_i} A_{T, x_i}$ and reconstruct the whole sequence with $B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_likely_hidden_viterbi(hidden_weights, visible_weights, initial_hidden, observations):\n",
    "    joint_probs = np.zeros((observations.size, initial_hidden.size), dtype=np.float64)\n",
    "    max_probs_ix = np.zeros((observations.size - 1, initial_hidden.size), dtype=np.int32)\n",
    "    \n",
    "    for i, observation in enumerate(observations):\n",
    "        if i == 0:\n",
    "            joint_probs[0] = initial_hidden * visible_weights[:, observation]    \n",
    "        else:\n",
    "            next_state_probs = joint_probs[i - 1, np.newaxis] * hidden_weights\n",
    "            next_state_argmax = next_state_probs.argmax(axis=1)\n",
    "            next_state_max = next_state_probs.max(axis=1)\n",
    "            \n",
    "            joint_probs[i] = next_state_max * visible_weights[:, observation]\n",
    "            max_probs_ix[i - 1] = next_state_argmax\n",
    "    \n",
    "    most_likely_path = np.zeros(observations.size, dtype=np.int32)\n",
    "    most_likely_path[-1] = joint_probs[-1].argmax()\n",
    "    for i in range(observations.size - 1, 0, -1):\n",
    "        most_likely_path[i - 1] = max_probs_ix[i - 1, most_likely_path[i]]\n",
    "    \n",
    "    return most_likely_path, joint_probs, max_probs_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[0 0 0 0 0]\n",
      "[[ 0.5  0. ]\n",
      " [ 0.5  0. ]\n",
      " [ 0.5  0. ]\n",
      " [ 0.5  0. ]\n",
      " [ 0.5  0. ]]\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "\n",
      "[[1 0 0 0 1 1 0 1]\n",
      " [1 0 0 1 0 1 1 1]]\n",
      "[0 0 0 0 0 0 0 0]\n",
      "[[  2.50000000e-01   2.50000000e-01]\n",
      " [  6.25000000e-02   6.25000000e-02]\n",
      " [  1.56250000e-02   1.56250000e-02]\n",
      " [  3.90625000e-03   3.90625000e-03]\n",
      " [  9.76562500e-04   9.76562500e-04]\n",
      " [  2.44140625e-04   2.44140625e-04]\n",
      " [  6.10351562e-05   6.10351562e-05]\n",
      " [  1.52587891e-05   1.52587891e-05]]\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "\n",
      "[[0 1 0 2 1 1 0 1]\n",
      " [2 1 0 3 1 1 3 0]]\n",
      "[2 1 0 3 1 0 2 1]\n",
      "[[  3.00000000e-02   1.00000000e-02   3.50000000e-01   2.50000000e-02]\n",
      " [  5.60000000e-02   6.30000000e-02   4.20000000e-02   3.50000000e-03]\n",
      " [  7.56000000e-03   1.26000000e-03   1.12000000e-02   4.72500000e-03]\n",
      " [  1.34400000e-03   0.00000000e+00   3.36000000e-04   1.47420000e-03]\n",
      " [  5.89680000e-05   5.30712000e-04   2.15040000e-04   4.42260000e-05]\n",
      " [  8.49139200e-05   9.55281600e-05   4.24569600e-05   1.59213600e-05]\n",
      " [  1.14633792e-05   1.91056320e-06   1.69827840e-05   7.16461200e-06]\n",
      " [  2.71724544e-06   3.05690112e-06   2.03793408e-06   3.43901376e-07]]\n",
      "[[2 2 2 2]\n",
      " [1 1 0 1]\n",
      " [2 2 2 0]\n",
      " [3 3 0 3]\n",
      " [1 1 1 1]\n",
      " [1 1 0 1]\n",
      " [2 2 2 0]]\n"
     ]
    }
   ],
   "source": [
    "def test_viterbi(hidden_weights, visible_weights, initial_hidden, length):\n",
    "    walk = walk_random(hidden_weights, visible_weights, initial_hidden)\n",
    "    observations_states = np.array(list(it.islice(walk, length))).T\n",
    "    print(observations_states)\n",
    "    print(*most_likely_hidden_viterbi(hidden_weights, visible_weights, initial_hidden, observations_states[0]), sep='\\n')\n",
    "\n",
    "test_viterbi(np.eye(2), np.eye(2), np.array([0.5, 0.5]), 5)\n",
    "print()\n",
    "\n",
    "test_viterbi(np.array([[0.5, 0.5], [0.5, 0.5]]), np.array([[0.5, 0.5], [0.5, 0.5]]), np.array([0.5, 0.5]), 8)\n",
    "print()\n",
    "\n",
    "test_viterbi(hidden_weights, visible_weights, initial_hidden, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM Observations\n",
    "\n",
    "It seems that in practice it's extremely common to use continuous visible states, with continuous distributions. Gaussiam Mixture Models seem to be a good model of speech. We can adapt the above implementation to estimate GMM. I already did EM for GMM in my EM notebook. The HMM model needs to be changed so that $v$ matrix does not contain probabilities for each emission, but has GMM parameters instead. The parameters should be used to calculate distribution and probabilities instead of them being explicitly given. Baum-Welch needs to update GMM parameters with their ML estimates.\n",
    "\n",
    "https://kastnerkyle.github.io/posts/single-speaker-word-recognition-with-hidden-markov-models/ - overview on how to use HMM for word recognition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
